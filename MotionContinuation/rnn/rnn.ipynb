{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyMbquhwrKKqlZa2WVxQGTAq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Drive"],"metadata":{"id":"B94WQE6TjIOf"}},{"cell_type":"code","source":["import sys\n","from google.colab import drive\n","\n","# mount google drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# insert directory\n","sys.path.insert(0, '/content/drive/MyDrive/CAS_AIArt/AIMovement/MotionContinuation')"],"metadata":{"id":"qCb88tlZjLCa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/CAS_AIArt/AIMovement/MotionContinuation/\n","%ls\n","%pwd"],"metadata":{"id":"YO6g1JrUjThJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Installation"],"metadata":{"id":"jZiezlhckCSN"}},{"cell_type":"code","source":["!pip install transforms3d\n","!pip install scipy==1.14.1\n","!pip install installers/fbx-2020.3.7-cp310-cp310-manylinux1_x86_64.whl"],"metadata":{"id":"NKtKo6FgkEn8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"6nKhW1wDi53O"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Awrx0vEo4SGH"},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torch import nn\n","from collections import OrderedDict\n","import networkx as nx\n","import scipy.linalg as sclinalg\n","\n","import os, sys, time, subprocess\n","import numpy as np\n","import math\n","\n","from common import utils\n","from common import bvh_tools as bvh\n","from common import fbx_tools as fbx\n","from common import mocap_tools as mocap\n","from common.quaternion import qmul, qrot, qnormalize_np, slerp\n","from common.pose_renderer import PoseRenderer"]},{"cell_type":"markdown","source":["# Device"],"metadata":{"id":"9zbxJTVSknDb"}},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print('Using {} device'.format(device))"],"metadata":{"id":"FjJ1n_s7kore"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Mocap Settings"],"metadata":{"id":"GV2U9HaelGwV"}},{"cell_type":"code","source":["mocap_file_path = \"data/\"\n","mocap_files = [\"ZacharyChant_Captury_Improvisation.fbx\"]\n","mocap_valid_frame_ranges = [ [ [ 1400, 29000 ] ] ]\n","mocap_pos_scale = 1.0\n","mocap_fps = 50"],"metadata":{"id":"SZal0s_clHuN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model Settings"],"metadata":{"id":"GHkUz3Lmlo47"}},{"cell_type":"code","source":["rnn_layer_dim = 512\n","rnn_layer_count = 2\n","\n","save_weights = True\n","load_weights = True\n","rnn_weights_file = \"results/weights/rnn_weights_epoch_200\""],"metadata":{"id":"CLkKEDxmlpwN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training settings"],"metadata":{"id":"BVB_plerluKi"}},{"cell_type":"code","source":["batch_size = 32\n","test_percentage = 0.1\n","\n","seq_input_length = 64\n","seq_output_length = 10 # this is only used for non-teacher forcing scenarios\n","\n","learning_rate = 1e-4\n","norm_loss_scale = 0.1\n","pos_loss_scale = 0.1\n","quat_loss_scale = 0.9\n","teacher_forcing_prob = 0.0\n","model_save_interval = 10\n","\n","epochs = 200\n","save_history = True\n","\n","# for skeletons with main body joints only\n","joint_loss_weights = [1.0]"],"metadata":{"id":"bHmYza9Llu-N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Visualization settings"],"metadata":{"id":"hRv8P-bPl3N2"}},{"cell_type":"code","source":["view_ele = 90.0\n","view_azi = -90.0\n","view_line_width = 1.0\n","view_size = 4.0"],"metadata":{"id":"N6h4VgFGl4NK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load Mocap Data"],"metadata":{"id":"cgsoYGjUl6UK"}},{"cell_type":"code","source":["bvh_tools = bvh.BVH_Tools()\n","fbx_tools = fbx.FBX_Tools()\n","mocap_tools = mocap.Mocap_Tools()\n","\n","all_mocap_data = []\n","\n","for mocap_file in mocap_files:\n","\n","    print(\"process file \", mocap_file)\n","\n","    if mocap_file.endswith(\".bvh\") or mocap_file.endswith(\".BVH\"):\n","        bvh_data = bvh_tools.load(mocap_file_path + \"/\" + mocap_file)\n","        mocap_data = mocap_tools.bvh_to_mocap(bvh_data)\n","    elif mocap_file.endswith(\".fbx\") or mocap_file.endswith(\".FBX\"):\n","        fbx_data = fbx_tools.load(mocap_file_path + \"/\" + mocap_file)\n","        mocap_data = mocap_tools.fbx_to_mocap(fbx_data)[0] # first skeleton only\n","\n","    mocap_data[\"skeleton\"][\"offsets\"] *= mocap_pos_scale\n","    mocap_data[\"motion\"][\"pos_local\"] *= mocap_pos_scale\n","\n","    # set x and z offset of root joint to zero\n","    mocap_data[\"skeleton\"][\"offsets\"][0, 0] = 0.0\n","    mocap_data[\"skeleton\"][\"offsets\"][0, 2] = 0.0\n","\n","    if mocap_file.endswith(\".bvh\") or mocap_file.endswith(\".BVH\"):\n","        mocap_data[\"motion\"][\"rot_local\"] = mocap_tools.euler_to_quat_bvh(mocap_data[\"motion\"][\"rot_local_euler\"], mocap_data[\"rot_sequence\"])\n","    elif mocap_file.endswith(\".fbx\") or mocap_file.endswith(\".FBX\"):\n","        mocap_data[\"motion\"][\"rot_local\"] = mocap_tools.euler_to_quat(mocap_data[\"motion\"][\"rot_local_euler\"], mocap_data[\"rot_sequence\"])\n","\n","    all_mocap_data.append(mocap_data)\n","\n","# retrieve mocap properties\n","\n","mocap_data = all_mocap_data[0]\n","joint_count = mocap_data[\"motion\"][\"rot_local\"].shape[1]\n","joint_dim = mocap_data[\"motion\"][\"rot_local\"].shape[2]\n","pose_dim = joint_count * joint_dim\n","\n","offsets = mocap_data[\"skeleton\"][\"offsets\"].astype(np.float32)\n","parents = mocap_data[\"skeleton\"][\"parents\"]\n","children = mocap_data[\"skeleton\"][\"children\"]\n","\n","# create edge list\n","def get_edge_list(children):\n","    edge_list = []\n","\n","    for parent_joint_index in range(len(children)):\n","        for child_joint_index in children[parent_joint_index]:\n","            edge_list.append([parent_joint_index, child_joint_index])\n","\n","    return edge_list\n","\n","edge_list = get_edge_list(children)"],"metadata":{"id":"Rz11Lwinl9kR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create Dataset"],"metadata":{"id":"ZV0aSmydo3sL"}},{"cell_type":"code","source":["X = []\n","y = []\n","\n","for i, mocap_data in enumerate(all_mocap_data):\n","\n","    print(\"mocap \", mocap_files[i])\n","\n","    pose_sequence = mocap_data[\"motion\"][\"rot_local\"]\n","    pose_sequence = np.reshape(pose_sequence, (-1, pose_dim))\n","\n","    print(\"shape \", pose_sequence.shape)\n","\n","    valid_frame_ranges = mocap_valid_frame_ranges[i]\n","\n","    for valid_frame_range in valid_frame_ranges:\n","\n","        frame_range_start = valid_frame_range[0]\n","        frame_range_end = valid_frame_range[1]\n","\n","        print(\"frame range from \", frame_range_start, \" to \", frame_range_end)\n","\n","        for pI in np.arange(frame_range_start, frame_range_end - seq_input_length - seq_output_length - 1):\n","\n","            X_sample = pose_sequence[pI:pI+seq_input_length]\n","            X.append(X_sample.reshape((seq_input_length, pose_dim)))\n","\n","            Y_sample = pose_sequence[pI+seq_input_length:pI+seq_input_length+seq_output_length]\n","            y.append(Y_sample.reshape((seq_output_length, pose_dim)))\n","\n","X = np.array(X)\n","y = np.array(y)\n","\n","X = torch.from_numpy(X).to(torch.float32)\n","y = torch.from_numpy(y).to(torch.float32)\n","\n","class SequenceDataset(Dataset):\n","    def __init__(self, X, y):\n","        self.X = X\n","        self.y = y\n","\n","    def __len__(self):\n","        return self.X.shape[0]\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx, ...], self.y[idx, ...]\n","\n","full_dataset = SequenceDataset(X, y)\n","\n","X_item, y_item = full_dataset[0]\n","\n","print(\"X_item s \", X_item.shape)\n","print(\"y_item s \", y_item.shape)\n","\n","test_size = int(test_percentage * len(full_dataset))\n","train_size = len(full_dataset) - test_size\n","\n","train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","X_batch, y_batch = next(iter(train_loader))\n","\n","print(\"X_batch s \", X_batch.shape)\n","print(\"y_batch s \", y_batch.shape)"],"metadata":{"id":"JHdhfF7Zo5R_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create Model"],"metadata":{"id":"pud_EWzgo8pX"}},{"cell_type":"code","source":["class Reccurent(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim, layer_count):\n","        super(Reccurent, self).__init__()\n","\n","        self.input_dim = input_dim\n","        self.hidden_dim = hidden_dim\n","        self.layer_count = layer_count\n","        self.output_dim = output_dim\n","\n","        rnn_layers = []\n","\n","        rnn_layers.append((\"rnn\", nn.LSTM(self.input_dim, self.hidden_dim, self.layer_count, batch_first=True)))\n","        self.rnn_layers = nn.Sequential(OrderedDict(rnn_layers))\n","\n","        dense_layers = []\n","        dense_layers.append((\"dense\", nn.Linear(self.hidden_dim, self.output_dim)))\n","        self.dense_layers = nn.Sequential(OrderedDict(dense_layers))\n","\n","    def forward(self, x):\n","        x, (_, _) = self.rnn_layers(x)\n","\n","        x = x[:, -1, :] # only last time step\n","        x = self.dense_layers(x)\n","\n","        return x\n","\n","rnn = Reccurent(pose_dim, rnn_layer_dim, pose_dim, rnn_layer_count).to(device)\n","print(rnn)\n","\n","# test Reccurent model\n","\n","batch_x, _ = next(iter(train_loader))\n","batch_x = batch_x.to(device)\n","\n","print(batch_x.shape)\n","\n","test_y2 = rnn(batch_x)\n","\n","print(test_y2.shape)\n","\n","if load_weights == True:\n","    rnn.load_state_dict(torch.load(rnn_weights_file))"],"metadata":{"id":"fryCxU-0pDCH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Forward Kinematics"],"metadata":{"id":"A-syCu1cJNRy"}},{"cell_type":"code","source":["def forward_kinematics(rotations, root_positions):\n","    \"\"\"\n","    Perform forward kinematics using the given trajectory and local rotations.\n","    Arguments (where N = batch size, L = sequence length, J = number of joints):\n","     -- rotations: (N, L, J, 4) tensor of unit quaternions describing the local rotations of each joint.\n","     -- root_positions: (N, L, 3) tensor describing the root joint positions.\n","    \"\"\"\n","\n","    assert len(rotations.shape) == 4\n","    assert rotations.shape[-1] == 4\n","\n","    toffsets = torch.tensor(offsets).to(device)\n","\n","    positions_world = []\n","    rotations_world = []\n","\n","    expanded_offsets = toffsets.expand(rotations.shape[0], rotations.shape[1], offsets.shape[0], offsets.shape[1])\n","\n","    # Parallelize along the batch and time dimensions\n","    for jI in range(offsets.shape[0]):\n","        if parents[jI] == -1:\n","            positions_world.append(root_positions)\n","            rotations_world.append(rotations[:, :, 0])\n","        else:\n","            positions_world.append(qrot(rotations_world[parents[jI]], expanded_offsets[:, :, jI]) \\\n","                                   + positions_world[parents[jI]])\n","            if len(children[jI]) > 0:\n","                rotations_world.append(qmul(rotations_world[parents[jI]], rotations[:, :, jI]))\n","            else:\n","                # This joint is a terminal node -> it would be useless to compute the transformation\n","                rotations_world.append(None)\n","\n","    return torch.stack(positions_world, dim=3).permute(0, 1, 3, 2)"],"metadata":{"id":"VgKoTvUuJPon"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"QI-3DIsSpJt9"}},{"cell_type":"code","source":["optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.336) # reduce the learning every 20 epochs by a factor of 10\n","\n","joint_loss_weights = torch.tensor(joint_loss_weights, dtype=torch.float32)\n","joint_loss_weights = joint_loss_weights.reshape(1, 1, -1).to(device)\n","\n","def norm_loss(yhat):\n","    _yhat = yhat.view(-1, 4)\n","    _norm = torch.norm(_yhat, dim=1)\n","    _diff = (_norm - 1.0) ** 2\n","    _loss = torch.mean(_diff)\n","    return _loss\n","\n","def pos_loss(y, yhat):\n","\n","    #print(\"pos_loss\")\n","    #print(\"y s \", y.shape)\n","    #print(\"yhat s \", yhat.shape)\n","\n","    # y and yhat shapes: batch_size, seq_length, pose_dim\n","\n","    # normalize tensors\n","    _yhat = yhat.view(-1, 4)\n","\n","    _yhat_norm = nn.functional.normalize(_yhat, p=2, dim=1)\n","    _y_rot = y.view((y.shape[0], y.shape[1], -1, 4))\n","    _yhat_rot = _yhat.view((y.shape[0], y.shape[1], -1, 4))\n","\n","    #print(\"_y_rot s \", _y_rot.shape)\n","    #print(\"_yhat_rot s \", _yhat_rot.shape)\n","\n","    zero_trajectory = torch.zeros((y.shape[0], y.shape[1], 3), dtype=torch.float32, requires_grad=True).to(device)\n","\n","    _y_pos = forward_kinematics(_y_rot, zero_trajectory)\n","    _yhat_pos = forward_kinematics(_yhat_rot, zero_trajectory)\n","\n","    #print(\"_y_pos s \", _y_pos.shape)\n","    #print(\"_yhat_pos s \", _yhat_pos.shape)\n","\n","    _pos_diff = torch.norm((_y_pos - _yhat_pos), dim=3)\n","\n","    #print(\"_pos_diff s \", _pos_diff.shape)\n","\n","    _pos_diff_weighted = _pos_diff * joint_loss_weights\n","\n","    _loss = torch.mean(_pos_diff_weighted)\n","\n","    return _loss\n","\n","def quat_loss(y, yhat):\n","\n","    #print(\"quat_loss\")\n","    #print(\"y s \", y.shape)\n","    #print(\"yhat s \", yhat.shape)\n","\n","    # y and yhat shapes: batch_size, seq_length, pose_dim\n","\n","    # normalize quaternion\n","\n","    _y = y.view((-1, 4))\n","    _yhat = yhat.view((-1, 4))\n","    _yhat_norm = nn.functional.normalize(_yhat, p=2, dim=1)\n","\n","    #print(\"_y s \", _y.shape)\n","    #print(\"_yhat_norm s \", _yhat_norm.shape)\n","\n","    # inverse of quaternion: https://www.mathworks.com/help/aeroblks/quaternioninverse.html\n","    _yhat_inv = _yhat_norm * torch.tensor([[1.0, -1.0, -1.0, -1.0]], dtype=torch.float32).to(device)\n","\n","    # calculate difference quaternion\n","    _diff = qmul(_yhat_inv, _y)\n","    # length of complex part\n","    _len = torch.norm(_diff[:, 1:], dim=1)\n","    # atan2\n","    _atan = torch.atan2(_len, _diff[:, 0])\n","    # abs\n","    _abs = torch.abs(_atan)\n","\n","    _abs = _abs.reshape(-1, 1, joint_count)\n","\n","    #print(\"_abs s \", _abs.shape)\n","\n","    _abs_weighted = _abs * joint_loss_weights\n","\n","    _loss = torch.mean(_abs_weighted)\n","    return _loss\n","\n","# autoencoder loss function\n","def loss(y, yhat):\n","    _norm_loss = norm_loss(yhat)\n","    _pos_loss = pos_loss(y, yhat)\n","    _quat_loss = quat_loss(y, yhat)\n","\n","    _total_loss = 0.0\n","    _total_loss += _norm_loss * norm_loss_scale\n","    _total_loss += _pos_loss * pos_loss_scale\n","    _total_loss += _quat_loss * quat_loss_scale\n","\n","    return _total_loss, _norm_loss, _pos_loss, _quat_loss\n","\n","def train_step(pose_sequences, target_poses, teacher_forcing):\n","\n","    rnn.train()\n","\n","    #print(\"ar_train_step\")\n","    #print(\"teacher_forcing \", teacher_forcing)\n","    #print(\"pose_sequences s \", pose_sequences.shape)\n","    #print(\"target_poses s \", target_poses.shape)\n","\n","    #_input_poses = pose_sequences.detach().clone()\n","    _input_poses = pose_sequences\n","    output_poses_length = target_poses.shape[1]\n","\n","    #print(\"output_poses_length \", output_poses_length)\n","\n","    _pred_poses_for_loss = []\n","    _target_poses_for_loss = []\n","\n","    for o_i in range(1, output_poses_length):\n","\n","        #print(\"_input_poses s \", _input_poses.shape)\n","\n","        _pred_poses = rnn(_input_poses)\n","        _pred_poses = torch.unsqueeze(_pred_poses, axis=1)\n","\n","        #print(\"_pred_poses s \", _pred_poses.shape)\n","\n","        _target_poses = target_poses[:,o_i,:].detach().clone()\n","        _target_poses = torch.unsqueeze(_target_poses, axis=1)\n","\n","        #print(\"_target_poses s \", _target_poses.shape)\n","\n","        _pred_poses_for_loss.append(_pred_poses)\n","        _target_poses_for_loss.append(_target_poses)\n","\n","        # shift input pose seqeunce one pose to the right\n","        # remove pose from beginning input pose sequence\n","        # detach necessary to avoid error concerning running backprob a second time\n","        _input_poses = _input_poses[:, 1:, :].detach().clone()\n","        _target_poses = _target_poses.detach().clone()\n","        _pred_poses = _pred_poses.detach().clone()\n","\n","        # add predicted or target pose to end of input pose sequence\n","        if teacher_forcing == True:\n","            _input_poses = torch.concat((_input_poses, _target_poses), axis=1)\n","        else:\n","            #_pred_poses = torch.reshape(_pred_poses, (_pred_poses.shape[0], 1, joint_count, joint_dim))\n","            _input_poses = torch.cat((_input_poses, _pred_poses), axis=1)\n","\n","        #print(\"_input_poses s \", _input_poses.shape)\n","\n","\n","        #print(\"_input_poses 2 s \", _input_poses.shape)\n","\n","    _pred_poses_for_loss = torch.cat(_pred_poses_for_loss, dim=1)\n","    _target_poses_for_loss = torch.cat(_target_poses_for_loss, dim=1)\n","\n","    #print(\"_pred_poses_for_loss 2 s \", _pred_poses_for_loss.shape)\n","    #print(\"_target_poses_for_loss 2 s \", _target_poses_for_loss.shape)\n","\n","    _loss, _norm_loss, _pos_loss, _quat_loss = loss(_target_poses_for_loss, _pred_poses_for_loss)\n","\n","    # Backpropagation\n","    optimizer.zero_grad()\n","    _loss.backward()\n","    optimizer.step()\n","\n","    #print(\"_ar_loss_total mean s \", _ar_loss_total.shape)\n","\n","    #return _ar_loss, _ar_norm_loss, _ar_quat_loss\n","\n","    return _loss, _norm_loss, _pos_loss, _quat_loss\n","\n","def test_step(pose_sequences, target_poses, teacher_forcing):\n","\n","    rnn.eval()\n","\n","    #print(\"ar_train_step\")\n","    #print(\"teacher_forcing \", teacher_forcing)\n","    #print(\"pose_sequences s \", pose_sequences.shape)\n","    #print(\"target_poses s \", target_poses.shape)\n","\n","    #_input_poses = pose_sequences.detach().clone()\n","    _input_poses = pose_sequences\n","    output_poses_length = target_poses.shape[1]\n","\n","    #print(\"output_poses_length \", output_poses_length)\n","\n","    _pred_poses_for_loss = []\n","    _target_poses_for_loss = []\n","\n","    with torch.no_grad():\n","\n","        for o_i in range(1, output_poses_length):\n","\n","            #print(\"_input_poses s \", _input_poses.shape)\n","\n","            _pred_poses = rnn(_input_poses)\n","            _pred_poses = torch.unsqueeze(_pred_poses, axis=1)\n","\n","            #print(\"_pred_poses s \", _pred_poses.shape)\n","\n","            _target_poses = target_poses[:,o_i,:].detach().clone()\n","            _target_poses = torch.unsqueeze(_target_poses, axis=1)\n","\n","            #print(\"_target_poses s \", _target_poses.shape)\n","\n","            _pred_poses_for_loss.append(_pred_poses)\n","            _target_poses_for_loss.append(_target_poses)\n","\n","            # shift input pose seqeunce one pose to the right\n","            # remove pose from beginning input pose sequence\n","            # detach necessary to avoid error concerning running backprob a second time\n","            _input_poses = _input_poses[:, 1:, :].detach().clone()\n","            _target_poses = _target_poses.detach().clone()\n","            _pred_poses = _pred_poses.detach().clone()\n","\n","            # add predicted or target pose to end of input pose sequence\n","            if teacher_forcing == True:\n","                _input_poses = torch.concat((_input_poses, _target_poses), axis=1)\n","            else:\n","                #_pred_poses = torch.reshape(_pred_poses, (_pred_poses.shape[0], 1, joint_count, joint_dim))\n","                _input_poses = torch.cat((_input_poses, _pred_poses), axis=1)\n","\n","            #print(\"_input_poses s \", _input_poses.shape)\n","\n","\n","            #print(\"_input_poses 2 s \", _input_poses.shape)\n","\n","        _pred_poses_for_loss = torch.cat(_pred_poses_for_loss, dim=1)\n","        _target_poses_for_loss = torch.cat(_target_poses_for_loss, dim=1)\n","\n","        #print(\"_pred_poses_for_loss 2 s \", _pred_poses_for_loss.shape)\n","        #print(\"_target_poses_for_loss 2 s \", _target_poses_for_loss.shape)\n","\n","        _loss, _norm_loss, _pos_loss, _quat_loss = loss(_target_poses_for_loss, _pred_poses_for_loss)\n","\n","    #print(\"_ar_loss_total mean s \", _ar_loss_total.shape)\n","\n","    #return _ar_loss, _ar_norm_loss, _ar_quat_loss\n","\n","    rnn.train()\n","\n","    return _loss, _norm_loss, _pos_loss, _quat_loss\n","\n","\n","def train(train_dataloader, test_dataloader, epochs):\n","\n","    loss_history = {}\n","    loss_history[\"train\"] = []\n","    loss_history[\"test\"] = []\n","    loss_history[\"norm\"] = []\n","    loss_history[\"pos\"] = []\n","    loss_history[\"quat\"] = []\n","\n","    for epoch in range(epochs):\n","        start = time.time()\n","\n","        _train_loss_per_epoch = []\n","        _norm_loss_per_epoch = []\n","        _pos_loss_per_epoch = []\n","        _quat_loss_per_epoch = []\n","\n","        for train_batch in train_dataloader:\n","            input_pose_sequences = train_batch[0].to(device)\n","            target_poses = train_batch[1].to(device)\n","\n","            use_teacher_forcing = np.random.uniform() < teacher_forcing_prob\n","\n","            _loss, _norm_loss, _pos_loss, _quat_loss = train_step(input_pose_sequences, target_poses, use_teacher_forcing)\n","\n","            _loss = _loss.detach().cpu().numpy()\n","            _norm_loss = _norm_loss.detach().cpu().numpy()\n","            _pos_loss = _pos_loss.detach().cpu().numpy()\n","            _quat_loss = _quat_loss.detach().cpu().numpy()\n","\n","            _train_loss_per_epoch.append(_loss)\n","            _norm_loss_per_epoch.append(_norm_loss)\n","            _pos_loss_per_epoch.append(_pos_loss)\n","            _quat_loss_per_epoch.append(_quat_loss)\n","\n","        _train_loss_per_epoch = np.mean(np.array(_train_loss_per_epoch))\n","        _norm_loss_per_epoch = np.mean(np.array(_norm_loss_per_epoch))\n","        _pos_loss_per_epoch = np.mean(np.array(_pos_loss_per_epoch))\n","        _quat_loss_per_epoch = np.mean(np.array(_quat_loss_per_epoch))\n","\n","        _test_loss_per_epoch = []\n","\n","        for test_batch in test_dataloader:\n","            input_pose_sequences = train_batch[0].to(device)\n","            target_poses = train_batch[1].to(device)\n","\n","            use_teacher_forcing = np.random.uniform() < teacher_forcing_prob\n","\n","            _loss, _, _, _ = test_step(input_pose_sequences, target_poses, use_teacher_forcing)\n","            #_loss, _, _ = test_step(input_pose_sequences, target_poses)\n","\n","            _loss = _loss.detach().cpu().numpy()\n","\n","            _test_loss_per_epoch.append(_loss)\n","\n","        _test_loss_per_epoch = np.mean(np.array(_test_loss_per_epoch))\n","\n","        if epoch % model_save_interval == 0 and save_weights == True:\n","            torch.save(rnn.state_dict(), \"results/weights/rnn_weights_epoch_{}\".format(epoch))\n","\n","        loss_history[\"train\"].append(_train_loss_per_epoch)\n","        loss_history[\"test\"].append(_test_loss_per_epoch)\n","        loss_history[\"norm\"].append(_norm_loss_per_epoch)\n","        loss_history[\"pos\"].append(_pos_loss_per_epoch)\n","        loss_history[\"quat\"].append(_quat_loss_per_epoch)\n","\n","        scheduler.step()\n","\n","        print ('epoch {} : train: {:01.4f} test: {:01.4f} norm {:01.4f} pos {:01.4f} quat {:01.4f} time {:01.2f}'.format(epoch + 1, _train_loss_per_epoch, _test_loss_per_epoch, _norm_loss_per_epoch, _pos_loss_per_epoch, _quat_loss_per_epoch, time.time()-start))\n","\n","    return loss_history\n","\n","# fit model\n","loss_history = train(train_loader, test_loader, epochs)"],"metadata":{"id":"SwRY09MFpLDg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Save Training"],"metadata":{"id":"ardq8p_iG4ox"}},{"cell_type":"code","source":["# save history\n","utils.save_loss_as_csv(loss_history, \"results/histories/rnn_history_{}.csv\".format(epochs))\n","utils.save_loss_as_image(loss_history, \"results/histories/rnn_history_{}.png\".format(epochs))\n","\n","# save model weights\n","torch.save(rnn.state_dict(), \"results/weights/rnn_weights_epoch_{}\".format(epochs))"],"metadata":{"id":"eL2JLDXiG51_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Inference and Rendering"],"metadata":{"id":"lBBaE3EyHFrD"}},{"cell_type":"code","source":["# inference and rendering\n","poseRenderer = PoseRenderer(edge_list)\n","\n","def export_sequence_anim(pose_sequence, file_name):\n","\n","    pose_count = pose_sequence.shape[0]\n","    pose_sequence = np.reshape(pose_sequence, (pose_count, joint_count, joint_dim))\n","\n","    pose_sequence = torch.tensor(np.expand_dims(pose_sequence, axis=0)).to(device)\n","    zero_trajectory = torch.tensor(np.zeros((1, pose_count, 3), dtype=np.float32)).to(device)\n","\n","    skel_sequence = forward_kinematics(pose_sequence, zero_trajectory)\n","\n","    skel_sequence = skel_sequence.detach().cpu().numpy()\n","    skel_sequence = np.squeeze(skel_sequence)\n","\n","    view_min, view_max = utils.get_equal_mix_max_positions(skel_sequence)\n","    skel_images = poseRenderer.create_pose_images(skel_sequence, view_min, view_max, view_ele, view_azi, view_line_width, view_size, view_size)\n","    skel_images[0].save(file_name, save_all=True, append_images=skel_images[1:], optimize=False, duration=33.0, loop=0)\n","\n","def export_sequence_bvh(pose_sequence, file_name):\n","\n","    pose_count = pose_sequence.shape[0]\n","\n","    pred_dataset = {}\n","    pred_dataset[\"frame_rate\"] = mocap_data[\"frame_rate\"]\n","    pred_dataset[\"rot_sequence\"] = mocap_data[\"rot_sequence\"]\n","    pred_dataset[\"skeleton\"] = mocap_data[\"skeleton\"]\n","    pred_dataset[\"motion\"] = {}\n","    pred_dataset[\"motion\"][\"pos_local\"] = np.repeat(np.expand_dims(pred_dataset[\"skeleton\"][\"offsets\"], axis=0), pose_count, axis=0)\n","    pred_dataset[\"motion\"][\"rot_local\"] = pose_sequence\n","    pred_dataset[\"motion\"][\"rot_local_euler\"] = mocap_tools.quat_to_euler_bvh(pred_dataset[\"motion\"][\"rot_local\"], pred_dataset[\"rot_sequence\"])\n","\n","    pred_bvh = mocap_tools.mocap_to_bvh(pred_dataset)\n","\n","    bvh_tools.write(pred_bvh, file_name)\n","\n","def export_sequence_fbx(pose_sequence, file_name):\n","\n","    pose_count = pose_sequence.shape[0]\n","\n","    pred_dataset = {}\n","    pred_dataset[\"frame_rate\"] = mocap_data[\"frame_rate\"]\n","    pred_dataset[\"rot_sequence\"] = mocap_data[\"rot_sequence\"]\n","    pred_dataset[\"skeleton\"] = mocap_data[\"skeleton\"]\n","    pred_dataset[\"motion\"] = {}\n","    pred_dataset[\"motion\"][\"pos_local\"] = np.repeat(np.expand_dims(pred_dataset[\"skeleton\"][\"offsets\"], axis=0), pose_count, axis=0)\n","    pred_dataset[\"motion\"][\"rot_local\"] = pose_sequence\n","    pred_dataset[\"motion\"][\"rot_local_euler\"] = mocap_tools.quat_to_euler(pred_dataset[\"motion\"][\"rot_local\"], pred_dataset[\"rot_sequence\"])\n","\n","    pred_fbx = mocap_tools.mocap_to_fbx([pred_dataset])\n","\n","    fbx_tools.write(pred_fbx, file_name)\n","\n","\n","def create_pred_sequence(pose_sequence, pose_count):\n","\n","    rnn.eval()\n","\n","    start_seq = pose_sequence\n","    start_seq = torch.from_numpy(start_seq).to(device)\n","    start_seq = torch.reshape(start_seq, (seq_input_length, pose_dim))\n","\n","    next_seq = start_seq\n","\n","    pred_poses = []\n","\n","    for i in range(pose_count):\n","\n","        with torch.no_grad():\n","            pred_pose = rnn(torch.unsqueeze(next_seq, axis=0))\n","\n","        # normalize pred pose\n","        pred_pose = torch.squeeze(pred_pose)\n","        pred_pose = pred_pose.reshape((-1, 4))\n","        pred_pose = nn.functional.normalize(pred_pose, p=2, dim=1)\n","        pred_pose = pred_pose.reshape((1, pose_dim))\n","\n","        pred_poses.append(pred_pose)\n","\n","        #print(\"next_seq s \", next_seq.shape)\n","        #print(\"pred_pose s \", pred_pose.shape)\n","\n","        next_seq = torch.cat([next_seq[1:,:], pred_pose], axis=0)\n","\n","    pred_poses = torch.cat(pred_poses, dim=0)\n","    pred_poses = pred_poses.reshape((pose_count, joint_count, joint_dim))\n","\n","    rnn.train()\n","\n","    return pred_poses.detach().cpu().numpy()\n","\n","# create original sequence\n","\n","seq_index = 0\n","seq_start = 1000\n","seq_length = 1000\n","\n","orig_sequence = all_mocap_data[seq_index][\"motion\"][\"rot_local\"].astype(np.float32)\n","\n","export_sequence_anim(orig_sequence[seq_start:seq_start+seq_length], \"results/anims/orig_sequence_seq_start_{}_length_{}.gif\".format(seq_start, seq_length))\n","export_sequence_fbx(orig_sequence[seq_start:seq_start+seq_length], \"results/anims/orig_sequence_seq_start_{}_length_{}.fbx\".format(seq_start, seq_length))\n","#export_sequence_bvh(orig_sequence[seq_start:seq_start+seq_length], \"results/anims/orig_sequence_seq_start_{}_length_{}.bvh\".format(seq_start, seq_length))\n","\n","\n","# create predicted sequence\n","\n","seq_index = 0\n","seq_start = 1000\n","seq_length = 1000\n","\n","orig_sequence = all_mocap_data[seq_index][\"motion\"][\"rot_local\"].astype(np.float32)\n","pred_sequence = create_pred_sequence(orig_sequence[seq_start:seq_start+seq_input_length], seq_length)\n","\n","export_sequence_anim(pred_sequence, \"results/anims/pred_sequence_epoch_{}_seq_start_{}_length_{}.gif\".format(epochs, seq_start, seq_length))\n","export_sequence_fbx(pred_sequence, \"results/anims/pred_sequence_epoch_{}_seq_start_{}_length_{}.fbx\".format(epochs, seq_start, seq_length))\n","#export_sequence_bvh(pred_sequence, \"results/anims/pred_sequence_epoch_{}_seq_start_{}_length_{}.bvh\".format(epochs, seq_start, seq_length))\n"],"metadata":{"id":"DZkbe_j1HHgk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip list"],"metadata":{"id":"vmFQNE9WHcDq"},"execution_count":null,"outputs":[]}]}