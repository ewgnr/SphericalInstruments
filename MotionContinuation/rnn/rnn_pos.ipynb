{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMV7HgBEDvQpl6tah8ACpme"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Drive"],"metadata":{"id":"zBihdMD0TAhV"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jXSIRUr7S9yv"},"outputs":[],"source":["import sys\n","from google.colab import drive\n","\n","# mount google drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# insert directory\n","sys.path.insert(0, '/content/drive/MyDrive/CAS_AIArt/AIMovement/MotionContinuation')"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/CAS_AIArt/AIMovement/MotionContinuation/\n","%ls\n","%pwd"],"metadata":{"id":"_TikhP5hTE1x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"Zo15KFy8TXnT"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torch import nn\n","from collections import OrderedDict\n","import networkx as nx\n","import scipy.linalg as sclinalg\n","\n","import os, sys, time, subprocess\n","import numpy as np\n","import math\n","import json\n","import pickle\n","\n","from common import utils\n","from common.pose_renderer import PoseRenderer"],"metadata":{"id":"_mRgexEQTldm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Device"],"metadata":{"id":"6Yhd5hwdTtQu"}},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print('Using {} device'.format(device))"],"metadata":{"id":"g4xF_j9pTuNw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Mocap Settings"],"metadata":{"id":"U5hVRWR7TyLk"}},{"cell_type":"code","source":["mocap_config_file = \"configs/Halpe26_config.json\"\n","mocap_file_path = \"data/\"\n","mocap_files = [\"HannahMartin_Pos2D_Performance.pkl\"]\n","mocap_valid_frame_ranges = [ [ [ 0, 3600 ] ] ]\n","mocap_sensor_ids = [\"/mocap/0/joint/pos2d_world\", \"/mocap/0/joint/visibility\"]\n","mocap_root_joint_name = \"Hip\"\n","mocap_fps = 30\n","mocap_joint_dim = 2"],"metadata":{"id":"VDJ4HHzYT6ye"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model Settings"],"metadata":{"id":"pDwvNMzQUH2m"}},{"cell_type":"code","source":["rnn_layer_dim = 512\n","rnn_layer_count = 2\n","\n","save_weights = True\n","load_weights = False\n","rnn_weights_file = \"results/weights/rnn_weights_epoch_200\""],"metadata":{"id":"gpjt4dJfUItb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training settings"],"metadata":{"id":"o1d4G5Q_UMHX"}},{"cell_type":"code","source":["batch_size = 32\n","test_percentage = 0.1\n","seq_input_length = 64\n","seq_output_length = 10 # this is only used for non-teacher forcing scenarios\n","learning_rate = 1e-4\n","pos_loss_scale = 1.0\n","teacher_forcing_prob = 0.0\n","model_save_interval = 10\n","epochs = 200\n","save_history = True\n","joint_loss_weights = [1.0]"],"metadata":{"id":"9hagSxRPUM-X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Visualization Settings"],"metadata":{"id":"PgYYxyt4UPf-"}},{"cell_type":"code","source":["if mocap_joint_dim == 2:\n","    view_ele = 90.0\n","    view_azi = 90.0\n","    view_line_width = 1.0\n","    view_size = 4.0\n","else:\n","    view_ele = 0.0\n","    view_azi = 0.0\n","    view_line_width = 1.0\n","    view_size = 4.0\n"],"metadata":{"id":"7cjIhoj4URET"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load Mocap Data"],"metadata":{"id":"VlPCwXDhUUvl"}},{"cell_type":"code","source":["# load mocap config\n","\n","with open(mocap_config_file) as f:\n","    mocap_config = json.load(f)\n","\n","\n","def config_to_skeletondata(mocap_config):\n","\n","    skeleton_data = {}\n","    skeleton_data[\"joints\"] = mocap_config[\"jointNames\"]\n","    skeleton_data[\"root\"] = skeleton_data[\"joints\"][0]\n","    skeleton_data[\"parents\"] = mocap_config[\"jointParents\"]\n","    skeleton_data[\"children\"] = mocap_config[\"jointChildren\"]\n","\n","    return skeleton_data\n","\n","def recording_to_motiondata(mocap_recording, skeleton_data, mocap_sensor_ids):\n","\n","    joint_count = len(skeleton_data[\"joints\"])\n","\n","    # gather sensor values\n","    motion_data = {}\n","\n","    sensor_ids = mocap_recording[\"sensor_ids\"]\n","    sensor_values = mocap_recording[\"sensor_values\"]\n","\n","    for sensor_id in mocap_sensor_ids:\n","\n","        #print(\"sensor_id \", sensor_id)\n","        motion_data[sensor_id]  = [ sensor_values [vI] for vI in range(len(sensor_values)) if sensor_ids[vI].endswith(sensor_id) ]\n","        motion_data[sensor_id] = np.array(motion_data[sensor_id], dtype=np.float32)\n","        motion_data[sensor_id] = np.reshape(motion_data[sensor_id], (motion_data[sensor_id].shape[0], joint_count, -1))\n","\n","    return motion_data\n","\n","skeleton_data = config_to_skeletondata(mocap_config)\n","\n","all_motion_data = []\n","\n","for mocap_file in mocap_files:\n","\n","    print(\"process file \", mocap_file)\n","\n","    with open(mocap_file_path + \"/\" + mocap_file, \"rb\") as f:\n","        mocap_recording = pickle.load(f)\n","\n","        motion_data = recording_to_motiondata(mocap_recording, skeleton_data, mocap_sensor_ids)\n","\n","        all_motion_data.append(motion_data)\n","\n","# retrieve mocap properties\n","\n","joint_count = len(skeleton_data[\"joints\"])\n","joint_dim = mocap_joint_dim\n","pose_dim = joint_count * joint_dim\n","\n","parents = skeleton_data[\"parents\"]\n","children = skeleton_data[\"children\"]\n","\n","joint_loss_weights = joint_loss_weights * joint_count\n","\n","# create edge list\n","def get_edge_list(children):\n","    edge_list = []\n","\n","    for parent_joint_index in range(len(children)):\n","        for child_joint_index in children[parent_joint_index]:\n","            edge_list.append([parent_joint_index, child_joint_index])\n","\n","    return edge_list\n","\n","edge_list = get_edge_list(children)\n","\n","# set root position to zero\n","mocap_root_joint_index = skeleton_data[\"joints\"].index(mocap_root_joint_name)\n","\n","for motion_data in all_motion_data:\n","\n","    if joint_dim == 3:\n","        joint_pos = motion_data[\"/mocap/0/joint/pos3d_world\"]\n","    else:\n","        joint_pos = motion_data[\"/mocap/0/joint/pos2d_world\"]\n","\n","    root_pos = joint_pos[:, mocap_root_joint_index:mocap_root_joint_index+1, :]\n","\n","    joint_pos_root_zero = joint_pos - root_pos\n","\n","    motion_data[\"/mocap/0/joint/pos_root_zero\"] = joint_pos_root_zero\n","\n","# calculate pose normalisation values\n","pose_sequence_all = []\n","for motion_data in all_motion_data:\n","    pose_sequence = motion_data[\"/mocap/0/joint/pos_root_zero\"]\n","    pose_sequence_all.append(pose_sequence)\n","\n","pose_sequence_all = np.concatenate(pose_sequence_all, axis=0)\n","#pose_sequence_all = np.reshape(pose_sequence_all, (-1, pose_dim))\n","\n","pose_mean = np.mean(pose_sequence_all, axis=0).flatten()\n","pose_std = np.std(pose_sequence_all, axis=0).flatten()"],"metadata":{"id":"wdnHJ3VnUW5D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create Dataset"],"metadata":{"id":"44OwUdgYUcPs"}},{"cell_type":"code","source":["X = []\n","y = []\n","\n","for i, motion_data in enumerate(all_motion_data):\n","\n","    print(\"mocap \", mocap_files[i])\n","\n","    pose_sequence = motion_data[\"/mocap/0/joint/pos_root_zero\"]\n","    pose_sequence = np.reshape(pose_sequence, (-1, pose_dim))\n","\n","    print(\"shape \", pose_sequence.shape)\n","\n","    valid_frame_ranges = mocap_valid_frame_ranges[i]\n","\n","    for valid_frame_range in valid_frame_ranges:\n","\n","        frame_range_start = valid_frame_range[0]\n","        frame_range_end = valid_frame_range[1]\n","\n","        print(\"frame range from \", frame_range_start, \" to \", frame_range_end)\n","\n","        for pI in np.arange(frame_range_start, frame_range_end - seq_input_length - seq_output_length - 1):\n","\n","            X_sample = pose_sequence[pI:pI+seq_input_length]\n","            X.append(X_sample.reshape((seq_input_length, pose_dim)))\n","\n","            Y_sample = pose_sequence[pI+seq_input_length:pI+seq_input_length+seq_output_length]\n","            y.append(Y_sample.reshape((seq_output_length, pose_dim)))\n","\n","X = np.array(X)\n","y = np.array(y)\n","\n","X = torch.from_numpy(X).to(torch.float32)\n","y = torch.from_numpy(y).to(torch.float32)\n","\n","class SequenceDataset(Dataset):\n","    def __init__(self, X, y):\n","        self.X = X\n","        self.y = y\n","\n","    def __len__(self):\n","        return self.X.shape[0]\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx, ...], self.y[idx, ...]\n","\n","full_dataset = SequenceDataset(X, y)\n","\n","X_item, y_item = full_dataset[0]\n","\n","print(\"X_item s \", X_item.shape)\n","print(\"y_item s \", y_item.shape)\n","\n","test_size = int(test_percentage * len(full_dataset))\n","train_size = len(full_dataset) - test_size\n","\n","train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","X_batch, y_batch = next(iter(train_loader))\n","\n","print(\"X_batch s \", X_batch.shape)\n","print(\"y_batch s \", y_batch.shape)"],"metadata":{"id":"t7vIwLc1UdKp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create Model"],"metadata":{"id":"VVKCKJL3Ugth"}},{"cell_type":"code","source":["class Reccurent(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim, layer_count):\n","        super(Reccurent, self).__init__()\n","\n","        self.input_dim = input_dim\n","        self.hidden_dim = hidden_dim\n","        self.layer_count = layer_count\n","        self.output_dim = output_dim\n","\n","        rnn_layers = []\n","\n","        rnn_layers.append((\"rnn\", nn.LSTM(self.input_dim, self.hidden_dim, self.layer_count, batch_first=True)))\n","        self.rnn_layers = nn.Sequential(OrderedDict(rnn_layers))\n","\n","        dense_layers = []\n","        dense_layers.append((\"dense\", nn.Linear(self.hidden_dim, self.output_dim)))\n","        self.dense_layers = nn.Sequential(OrderedDict(dense_layers))\n","\n","    def forward(self, x):\n","        x, (_, _) = self.rnn_layers(x)\n","\n","        x = x[:, -1, :] # only last time step\n","        x = self.dense_layers(x)\n","\n","        return x\n","\n","rnn = Reccurent(pose_dim, rnn_layer_dim, pose_dim, rnn_layer_count).to(device)\n","print(rnn)\n","\n","# test Reccurent model\n","\n","batch_x, _ = next(iter(train_loader))\n","batch_x = batch_x.to(device)\n","\n","print(batch_x.shape)\n","\n","test_y2 = rnn(batch_x)\n","\n","print(test_y2.shape)\n","\n","if load_weights == True:\n","    rnn.load_state_dict(torch.load(rnn_weights_file))"],"metadata":{"id":"fDPvlTVDUilD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"VNIHkPB7UmNV"}},{"cell_type":"code","source":["optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.336) # reduce the learning every 20 epochs by a factor of 10\n","\n","# pose mean and std\n","\n","pose_mean = torch.tensor(pose_mean).reshape(1, 1, -1).to(device)\n","pose_std = torch.tensor(pose_std).reshape(1, 1, -1).to(device)\n","\n","# joint loss weights\n","\n","joint_loss_weights = torch.tensor(joint_loss_weights, dtype=torch.float32)\n","joint_loss_weights = joint_loss_weights.reshape(1, 1, -1).to(device)\n","\n","def pos_loss(y, yhat):\n","    # y and yhat shapes: batch_size, seq_length, pose_dim\n","\n","    #print(\"pos_loss\")\n","    #print(\"y s \", y.shape)\n","    #print(\"yhat s \", yhat.shape)\n","\n","    # reshape into batch_size x sequence_length x joint_count x joint_dim\n","    _y = y.reshape(y.shape[0], y.shape[1], joint_count, joint_dim )\n","    _yhat = yhat.reshape(y.shape[0], y.shape[1], joint_count, joint_dim )\n","\n","    #print(\"_y s \", _y.shape)\n","    #print(\"_yhat s \", _yhat.shape)\n","\n","    _pos_diff = torch.norm((_y - _yhat), dim=3)\n","\n","    #print(\"_pos_diff s \", _pos_diff.shape)\n","    #print(\"joint_loss_weights s \", joint_loss_weights.shape)\n","\n","    _pos_diff_weighted = _pos_diff * joint_loss_weights\n","\n","    _loss = torch.mean(_pos_diff_weighted)\n","\n","    return _loss\n","\n","\n","# position loss function\n","def loss(y, yhat):\n","    _pos_loss = pos_loss(y, yhat)\n","\n","    _total_loss = 0.0\n","    _total_loss += _pos_loss * pos_loss_scale\n","\n","    return _total_loss, _pos_loss\n","\n","def train_step(pose_sequences, target_poses, teacher_forcing):\n","\n","    rnn.train()\n","\n","    #print(\"ar_train_step\")\n","    #print(\"teacher_forcing \", teacher_forcing)\n","    #print(\"pose_sequences s \", pose_sequences.shape)\n","    #print(\"target_poses s \", target_poses.shape)\n","\n","    #_input_poses = pose_sequences.detach().clone()\n","    _input_poses = pose_sequences\n","    _input_poses_norm = (_input_poses - pose_mean) / pose_std\n","    _input_poses_norm = torch.nan_to_num(_input_poses_norm)\n","\n","    target_poses_norm = (target_poses - pose_mean) / pose_std\n","    target_poses_norm = torch.nan_to_num(target_poses_norm)\n","\n","    output_poses_length = target_poses_norm.shape[1]\n","\n","    #print(\"output_poses_length \", output_poses_length)\n","\n","    _pred_poses_norm_for_loss = []\n","    _target_poses_norm_for_loss = []\n","\n","    for o_i in range(1, output_poses_length):\n","\n","        #print(\"_input_poses_norm s \", _input_poses_norm.shape)\n","\n","        _pred_poses_norm = rnn(_input_poses_norm)\n","        _pred_poses_norm = torch.unsqueeze(_pred_poses_norm, axis=1)\n","\n","        #print(\"_pred_poses s \", _pred_poses.shape)\n","\n","        _target_poses_norm = target_poses_norm[:,o_i,:].detach().clone()\n","        _target_poses_norm = torch.unsqueeze(_target_poses_norm, axis=1)\n","\n","        #print(\"_target_poses_norm s \", _target_poses_norm.shape)\n","\n","        _pred_poses_norm_for_loss.append(_pred_poses_norm)\n","        _target_poses_norm_for_loss.append(_target_poses_norm)\n","\n","        # shift input pose seqeunce one pose to the right\n","        # remove pose from beginning input pose sequence\n","        # detach necessary to avoid error concerning running backprob a second time\n","        _input_poses_norm = _input_poses_norm[:, 1:, :].detach().clone()\n","        _target_poses_norm = _target_poses_norm.detach().clone()\n","        _pred_poses_norm = _pred_poses_norm.detach().clone()\n","\n","        # add predicted or target pose to end of input pose sequence\n","        if teacher_forcing == True:\n","            _input_poses_norm = torch.concat((_input_poses_norm, _target_poses_norm), axis=1)\n","        else:\n","            #_pred_poses = torch.reshape(_pred_poses, (_pred_poses.shape[0], 1, joint_count, joint_dim))\n","            _input_poses_norm = torch.cat((_input_poses_norm, _pred_poses_norm), axis=1)\n","\n","        #print(\"_input_poses s \", _input_poses.shape)\n","\n","\n","        #print(\"_input_poses 2 s \", _input_poses.shape)\n","\n","    _pred_poses_norm_for_loss = torch.cat(_pred_poses_norm_for_loss, dim=1)\n","    _target_poses_norm_for_loss = torch.cat(_target_poses_norm_for_loss, dim=1)\n","\n","    #print(\"_pred_poses_for_loss 2 s \", _pred_poses_for_loss.shape)\n","    #print(\"_target_poses_for_loss 2 s \", _target_poses_for_loss.shape)\n","\n","    _loss, _pos_loss = loss(_target_poses_norm_for_loss, _pred_poses_norm_for_loss)\n","\n","    # Backpropagation\n","    optimizer.zero_grad()\n","    _loss.backward()\n","    optimizer.step()\n","\n","    #print(\"_ar_loss_total mean s \", _ar_loss_total.shape)\n","\n","    #return _ar_loss, _ar_norm_loss, _ar_quat_loss\n","\n","    return _loss, _pos_loss\n","\n","def test_step(pose_sequences, target_poses, teacher_forcing):\n","\n","    rnn.eval()\n","\n","    #print(\"ar_train_step\")\n","    #print(\"teacher_forcing \", teacher_forcing)\n","    #print(\"pose_sequences s \", pose_sequences.shape)\n","    #print(\"target_poses s \", target_poses.shape)\n","\n","    #_input_poses = pose_sequences.detach().clone()\n","    _input_poses = pose_sequences\n","    _input_poses_norm = (_input_poses - pose_mean) / pose_std\n","    _input_poses_norm = torch.nan_to_num(_input_poses_norm)\n","\n","    target_poses_norm = (target_poses - pose_mean) / pose_std\n","    target_poses_norm = torch.nan_to_num(target_poses_norm)\n","\n","    output_poses_length = target_poses_norm.shape[1]\n","\n","    #print(\"output_poses_length \", output_poses_length)\n","\n","    _pred_poses_norm_for_loss = []\n","    _target_poses_norm_for_loss = []\n","\n","    with torch.no_grad():\n","\n","        for o_i in range(1, output_poses_length):\n","\n","            #print(\"_input_poses_norm s \", _input_poses_norm.shape)\n","\n","            _pred_poses_norm = rnn(_input_poses_norm)\n","            _pred_poses_norm = torch.unsqueeze(_pred_poses_norm, axis=1)\n","\n","            #print(\"_pred_poses s \", _pred_poses.shape)\n","\n","            _target_poses_norm = target_poses_norm[:,o_i,:].detach().clone()\n","            _target_poses_norm = torch.unsqueeze(_target_poses_norm, axis=1)\n","\n","            #print(\"_target_poses_norm s \", _target_poses_norm.shape)\n","\n","            _pred_poses_norm_for_loss.append(_pred_poses_norm)\n","            _target_poses_norm_for_loss.append(_target_poses_norm)\n","\n","            # shift input pose seqeunce one pose to the right\n","            # remove pose from beginning input pose sequence\n","            # detach necessary to avoid error concerning running backprob a second time\n","            _input_poses_norm = _input_poses_norm[:, 1:, :].detach().clone()\n","            _target_poses_norm = _target_poses_norm.detach().clone()\n","            _pred_poses_norm = _pred_poses_norm.detach().clone()\n","\n","            # add predicted or target pose to end of input pose sequence\n","            if teacher_forcing == True:\n","                _input_poses_norm = torch.concat((_input_poses_norm, _target_poses_norm), axis=1)\n","            else:\n","                #_pred_poses = torch.reshape(_pred_poses, (_pred_poses.shape[0], 1, joint_count, joint_dim))\n","                _input_poses_norm = torch.cat((_input_poses_norm, _pred_poses_norm), axis=1)\n","\n","            #print(\"_input_poses s \", _input_poses.shape)\n","\n","\n","            #print(\"_input_poses 2 s \", _input_poses.shape)\n","\n","        _pred_poses_norm_for_loss = torch.cat(_pred_poses_norm_for_loss, dim=1)\n","        _target_poses_norm_for_loss = torch.cat(_target_poses_norm_for_loss, dim=1)\n","\n","        #print(\"_pred_poses_for_loss 2 s \", _pred_poses_for_loss.shape)\n","        #print(\"_target_poses_for_loss 2 s \", _target_poses_for_loss.shape)\n","\n","        _loss, _pos_loss = loss(_target_poses_norm_for_loss, _pred_poses_norm_for_loss)\n","\n","    #print(\"_ar_loss_total mean s \", _ar_loss_total.shape)\n","\n","    #return _ar_loss, _ar_norm_loss, _ar_quat_loss\n","\n","    rnn.train()\n","\n","    return _loss, _pos_loss\n","\n","def train(train_dataloader, test_dataloader, epochs):\n","\n","    loss_history = {}\n","    loss_history[\"train\"] = []\n","    loss_history[\"test\"] = []\n","    loss_history[\"pos\"] = []\n","\n","    for epoch in range(epochs):\n","        start = time.time()\n","\n","        _train_loss_per_epoch = []\n","        _pos_loss_per_epoch = []\n","\n","        for train_batch in train_dataloader:\n","            input_pose_sequences = train_batch[0].to(device)\n","            target_poses = train_batch[1].to(device)\n","\n","            use_teacher_forcing = np.random.uniform() < teacher_forcing_prob\n","\n","            _loss, _pos_loss = train_step(input_pose_sequences, target_poses, use_teacher_forcing)\n","\n","            _loss = _loss.detach().cpu().numpy()\n","            _pos_loss = _pos_loss.detach().cpu().numpy()\n","\n","            _train_loss_per_epoch.append(_loss)\n","            _pos_loss_per_epoch.append(_pos_loss)\n","\n","        _train_loss_per_epoch = np.mean(np.array(_train_loss_per_epoch))\n","        _pos_loss_per_epoch = np.mean(np.array(_pos_loss_per_epoch))\n","\n","        _test_loss_per_epoch = []\n","\n","        for test_batch in test_dataloader:\n","            input_pose_sequences = train_batch[0].to(device)\n","            target_poses = train_batch[1].to(device)\n","\n","            use_teacher_forcing = np.random.uniform() < teacher_forcing_prob\n","\n","            _loss, _ = test_step(input_pose_sequences, target_poses, use_teacher_forcing)\n","\n","            _loss = _loss.detach().cpu().numpy()\n","\n","            _test_loss_per_epoch.append(_loss)\n","\n","        _test_loss_per_epoch = np.mean(np.array(_test_loss_per_epoch))\n","\n","        if epoch % model_save_interval == 0 and save_weights == True:\n","            torch.save(rnn.state_dict(), \"results/weights/rnn_weights_epoch_{}\".format(epoch))\n","\n","        loss_history[\"train\"].append(_train_loss_per_epoch)\n","        loss_history[\"test\"].append(_test_loss_per_epoch)\n","        loss_history[\"pos\"].append(_pos_loss_per_epoch)\n","\n","        scheduler.step()\n","\n","        print ('epoch {} : train: {:01.4f} test: {:01.4f} pos {:01.4f} time {:01.2f}'.format(epoch + 1, _train_loss_per_epoch, _test_loss_per_epoch, _pos_loss_per_epoch, time.time()-start))\n","\n","    return loss_history\n","\n","# fit model\n","loss_history = train(train_loader, test_loader, epochs)"],"metadata":{"id":"o54W5HyZUnM9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Save Training"],"metadata":{"id":"kWbCG8vSu3FS"}},{"cell_type":"code","source":["# save history\n","utils.save_loss_as_csv(loss_history, \"results/histories/rnn_history_{}.csv\".format(epochs))\n","utils.save_loss_as_image(loss_history, \"results/histories/rnn_history_{}.png\".format(epochs))\n","\n","# save model weights\n","torch.save(rnn.state_dict(), \"results/weights/rnn_weights_epoch_{}\".format(epochs))\n"],"metadata":{"id":"Tsmbx92yu4Gy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Inference and Rendering"],"metadata":{"id":"QVNQ_ONOu6_O"}},{"cell_type":"code","source":["poseRenderer = PoseRenderer(edge_list)\n","\n","def export_sequence_anim(pose_sequence, file_name):\n","\n","    pose_count = pose_sequence.shape[0]\n","    pose_sequence = np.reshape(pose_sequence, (pose_count, joint_count, joint_dim))\n","\n","    if joint_dim == 2: # add third axis to pose sequence\n","        pose_sequence = np.concatenate([pose_sequence, np.zeros((pose_count, joint_count, 1))], axis=2)\n","\n","    skel_sequence = pose_sequence\n","\n","    view_min, view_max = utils.get_equal_mix_max_positions(skel_sequence)\n","    skel_images = poseRenderer.create_pose_images(skel_sequence, view_min, view_max, view_ele, view_azi, view_line_width, view_size, view_size)\n","    skel_images[0].save(file_name, save_all=True, append_images=skel_images[1:], optimize=False, duration=33.0, loop=0)\n","\n","def export_sequence_pkl(pose_sequence, file_name):\n","\n","    pose_count = pose_sequence.shape[0]\n","    pose_sequence = np.reshape(pose_sequence, (pose_count, pose_dim))\n","\n","    export_dict = {}\n","    export_dict[\"class_id\"] = 0\n","    if joint_dim == 2:\n","        export_dict[\"sensor_ids\"] = [\"/mocap/0/joint/pos2d_world\"] * pose_count\n","    else:\n","        export_dict[\"sensor_ids\"] = [\"/mocap/0/joint/pos3d_world\"] * pose_count\n","\n","    # this should be a list of tuples, but I guess a list of lists is also fine\n","    export_dict[\"sensor_values\"] = pose_sequence.tolist()\n","    export_dict[\"time_stamps\"] = (np.arange(0, pose_count, 1) * (1.0 / mocap_fps)).tolist()\n","\n","    with open(file_name, \"wb\") as f:\n","        pickle.dump(export_dict, f)\n","\n","def create_pred_sequence(pose_sequence, pose_count):\n","\n","    rnn.eval()\n","\n","    start_seq = pose_sequence\n","    start_seq = torch.from_numpy(start_seq).to(device)\n","    start_seq = torch.reshape(start_seq, (seq_input_length, pose_dim))\n","\n","    next_seq = start_seq\n","\n","    pred_poses = []\n","\n","    for i in range(pose_count):\n","\n","        with torch.no_grad():\n","\n","            next_seq_norm = (torch.unsqueeze(next_seq, axis=0) - pose_mean) / pose_std\n","            next_seq_norm = torch.nan_to_num(next_seq_norm)\n","\n","            #print(\"next_seq_norm s \", next_seq_norm.shape)\n","\n","            pred_pose_norm = rnn(next_seq_norm)\n","\n","            #print(\"pred_pose_norm s \", pred_pose_norm.shape)\n","\n","            pred_pose_norm = torch.unsqueeze(pred_pose_norm, axis=0)\n","            pred_pose = pred_pose_norm * pose_std + pose_mean\n","            pred_pose = torch.squeeze(pred_pose, axis=0)\n","\n","            #print(\"pred_pose s \", pred_pose.shape)\n","\n","        pred_poses.append(pred_pose)\n","\n","        #print(\"next_seq s \", next_seq.shape)\n","        #print(\"pred_pose s \", pred_pose.shape)\n","\n","        next_seq = torch.cat([next_seq[1:,:], pred_pose], axis=0)\n","\n","    pred_poses = torch.cat(pred_poses, dim=0)\n","    pred_poses = pred_poses.reshape((pose_count, joint_count, joint_dim))\n","\n","    pred_poses = pred_poses.detach().cpu().numpy()\n","\n","    rnn.train()\n","\n","    return pred_poses\n","\n","# create original sequence\n","\n","seq_index = 0\n","seq_start = 1000\n","seq_length = 1000\n","\n","orig_sequence = all_motion_data[seq_index][\"/mocap/0/joint/pos_root_zero\"].astype(np.float32)\n","\n","export_sequence_anim(orig_sequence[seq_start:seq_start+seq_length], \"results/anims/orig_sequence_seq_start_{}_length_{}.gif\".format(seq_start, seq_length))\n","export_sequence_pkl(orig_sequence[seq_start:seq_start+seq_length], \"results/anims/orig_sequence_seq_start_{}_length_{}.pkl\".format(seq_start, seq_length))\n","\n","# create predicted sequence\n","\n","seq_index = 0\n","seq_start = 1000\n","seq_length = 1000\n","\n","orig_sequence = all_motion_data[seq_index][\"/mocap/0/joint/pos_root_zero\"].astype(np.float32)\n","pred_sequence = create_pred_sequence(orig_sequence[seq_start:seq_start+seq_input_length], seq_length)\n","\n","export_sequence_anim(pred_sequence, \"results/anims/pred_sequence_epoch_{}_seq_start_{}_length_{}.gif\".format(epochs, seq_start, seq_length))\n","export_sequence_pkl(pred_sequence, \"results/anims/pred_sequence_epoch_{}_seq_start_{}_length_{}.pkl\".format(epochs, seq_start, seq_length))\n"],"metadata":{"id":"cyYe8Mo6vAjA"},"execution_count":null,"outputs":[]}]}