{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMDbakEZ7fFlNpm0ALjcrjr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Drive"],"metadata":{"id":"3vF9Gyy0tOO8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"y6kEur1ztHBF"},"outputs":[],"source":["import sys\n","from google.colab import drive\n","\n","# mount google drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# insert directory\n","sys.path.insert(0, '/content/drive/MyDrive/CAS_AIArt/AIMovement/MotionTransformation/VAE-RNN')"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/CAS_AIArt/AIMovement/MotionTransformation/VAE-RNN/\n","%ls\n","%pwd"],"metadata":{"id":"N3FyLvdVtav4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Installation"],"metadata":{"id":"gf1VsmT4tkWy"}},{"cell_type":"code","source":["!pip install transforms3d\n","!pip install scipy==1.14.1\n","!pip install installers/fbx-2020.3.7-cp310-cp310-manylinux1_x86_64.whl"],"metadata":{"id":"zSNzjZI7tmpX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"VEw3oq3AttLY"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torch import nn\n","from collections import OrderedDict\n","\n","import os, sys, time, subprocess\n","import numpy as np\n","\n","from common import utils\n","from common import bvh_tools as bvh\n","from common import fbx_tools as fbx\n","from common import mocap_tools as mocap\n","from common.quaternion import qmul, qrot, qnormalize_np, slerp, qfix\n","from common.pose_renderer import PoseRenderer\n","\n","from sklearn.manifold import TSNE\n","from matplotlib import pyplot as plt"],"metadata":{"id":"Mwu4Jzqgtulg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Device"],"metadata":{"id":"DG-3vDa8ty4D"}},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print('Using {} device'.format(device))"],"metadata":{"id":"7BDgk0xCt0zc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Mocap Settings"],"metadata":{"id":"kv64Vv9Zt3Ym"}},{"cell_type":"code","source":["mocap_file_path = \"data/\"\n","mocap_files = [\"ZacharyChant_Captury_Improvisation.fbx\"]\n","mocap_valid_frame_ranges = [ [ [ 1400, 29000 ] ] ]\n","mocap_pos_scale = 0.1\n","mocap_fps = 50"],"metadata":{"id":"xisvxJXMt4WN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model Settings"],"metadata":{"id":"eHUUIlvEt8wo"}},{"cell_type":"code","source":["latent_dim = 32\n","sequence_length = 64\n","ae_rnn_layer_count = 2\n","ae_rnn_layer_size = 512\n","ae_dense_layer_sizes = [ 512 ]\n","\n","save_models = False\n","save_tscript = False\n","save_weights = True\n","\n","# load model weights\n","load_weights = False\n","encoder_weights_file = \"results/weights/encoder_weights_epoch_600\"\n","decoder_weights_file = \"results/weights/decoder_weights_epoch_600\""],"metadata":{"id":"cLdsyMent-DA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training Settings"],"metadata":{"id":"6dR9oUpjuCCe"}},{"cell_type":"code","source":["sequence_offset = 2 # when creating sequence excerpts, each excerpt is offset from the previous one by this value\n","batch_size = 16\n","train_percentage = 0.8 # train / test split\n","test_percentage  = 0.2\n","ae_learning_rate = 1e-4\n","ae_norm_loss_scale = 0.1\n","ae_pos_loss_scale = 0.1\n","ae_quat_loss_scale = 1.0\n","ae_kld_loss_scale = 0.0 # will be calculated\n","kld_scale_cycle_duration = 100\n","kld_scale_min_const_duration = 20\n","kld_scale_max_const_duration = 20\n","min_kld_scale = 0.0\n","max_kld_scale = 0.1\n","\n","epochs = 600\n","model_save_interval = 50\n","save_history = True\n","\n","# for skeletons with main body joints only\n","joint_loss_weights = [1.0]"],"metadata":{"id":"ngpEqxL7uC4C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Visualization Settings"],"metadata":{"id":"rHleDuWeuIOl"}},{"cell_type":"code","source":["view_ele = 90.0\n","view_azi = -90.0\n","view_line_width = 1.0\n","view_size = 4.0"],"metadata":{"id":"U8KVENKuuJAC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load Mocap Data"],"metadata":{"id":"WZWKB5jiuLxV"}},{"cell_type":"code","source":["bvh_tools = bvh.BVH_Tools()\n","fbx_tools = fbx.FBX_Tools()\n","mocap_tools = mocap.Mocap_Tools()\n","\n","all_mocap_data = []\n","\n","for mocap_file in mocap_files:\n","\n","    print(\"process file \", mocap_file)\n","\n","    if mocap_file.endswith(\".bvh\") or mocap_file.endswith(\".BVH\"):\n","        bvh_data = bvh_tools.load(mocap_file_path + \"/\" + mocap_file)\n","        mocap_data = mocap_tools.bvh_to_mocap(bvh_data)\n","    elif mocap_file.endswith(\".fbx\") or mocap_file.endswith(\".FBX\"):\n","        fbx_data = fbx_tools.load(mocap_file_path + \"/\" + mocap_file)\n","        mocap_data = mocap_tools.fbx_to_mocap(fbx_data)[0] # first skeleton only\n","\n","    mocap_data[\"skeleton\"][\"offsets\"] *= mocap_pos_scale\n","    mocap_data[\"motion\"][\"pos_local\"] *= mocap_pos_scale\n","\n","    # set x and z offset of root joint to zero\n","    mocap_data[\"skeleton\"][\"offsets\"][0, 0] = 0.0\n","    mocap_data[\"skeleton\"][\"offsets\"][0, 2] = 0.0\n","\n","    if mocap_file.endswith(\".bvh\") or mocap_file.endswith(\".BVH\"):\n","        mocap_data[\"motion\"][\"rot_local\"] = mocap_tools.euler_to_quat_bvh(mocap_data[\"motion\"][\"rot_local_euler\"], mocap_data[\"rot_sequence\"])\n","    elif mocap_file.endswith(\".fbx\") or mocap_file.endswith(\".FBX\"):\n","        mocap_data[\"motion\"][\"rot_local\"] = mocap_tools.euler_to_quat(mocap_data[\"motion\"][\"rot_local_euler\"], mocap_data[\"rot_sequence\"])\n","\n","    all_mocap_data.append(mocap_data)\n","\n","# retrieve mocap properties\n","\n","mocap_data = all_mocap_data[0]\n","joint_count = mocap_data[\"motion\"][\"rot_local\"].shape[1]\n","joint_dim = mocap_data[\"motion\"][\"rot_local\"].shape[2]\n","pose_dim = joint_count * joint_dim\n","\n","offsets = mocap_data[\"skeleton\"][\"offsets\"].astype(np.float32)\n","parents = mocap_data[\"skeleton\"][\"parents\"]\n","children = mocap_data[\"skeleton\"][\"children\"]\n","\n","# create edge list\n","def get_edge_list(children):\n","    edge_list = []\n","\n","    for parent_joint_index in range(len(children)):\n","        for child_joint_index in children[parent_joint_index]:\n","            edge_list.append([parent_joint_index, child_joint_index])\n","\n","    return edge_list\n","\n","edge_list = get_edge_list(children)"],"metadata":{"id":"SwWw3NuguMox"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create Dataset"],"metadata":{"id":"P5Mn9u7-uRIX"}},{"cell_type":"code","source":["# gather pose sequence excerpts\n","\n","pose_sequence_excerpts = []\n","\n","for mocap_data in all_mocap_data:\n","    pose_sequence = mocap_data[\"motion\"][\"rot_local\"]\n","    pose_sequence = np.reshape(pose_sequence, (-1, pose_dim))\n","\n","    frame_range_start = 0\n","    frame_range_end = pose_sequence.shape[0]\n","\n","    for seq_excerpt_start in np.arange(frame_range_start, frame_range_end - sequence_length, sequence_offset):\n","        #print(\"valid: start \", frame_range_start, \" end \", frame_range_end, \" exc: start \", seq_excerpt_start, \" end \", (seq_excerpt_start + sequence_length) )\n","        pose_sequence_excerpt =  pose_sequence[seq_excerpt_start:seq_excerpt_start + sequence_length]\n","        pose_sequence_excerpts.append(pose_sequence_excerpt)\n","\n","pose_sequence_excerpts = np.array(pose_sequence_excerpts, dtype=np.float32)\n","\n","# create dataset\n","\n","sequence_excerpts_count = pose_sequence_excerpts.shape[0]\n","\n","class SequenceDataset(Dataset):\n","    def __init__(self, sequence_excerpts):\n","        self.sequence_excerpts = sequence_excerpts\n","\n","    def __len__(self):\n","        return self.sequence_excerpts.shape[0]\n","\n","    def __getitem__(self, idx):\n","        return self.sequence_excerpts[idx, ...]\n","\n","\n","full_dataset = SequenceDataset(pose_sequence_excerpts)\n","dataset_size = len(full_dataset)\n","\n","test_size = int(test_percentage * dataset_size)\n","train_size = dataset_size - test_size\n","\n","train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"],"metadata":{"id":"lN2Z1vT5uR_L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create Models"],"metadata":{"id":"aRwUNfSguVIK"}},{"cell_type":"code","source":["# create encoder model\n","\n","class Encoder(nn.Module):\n","    def __init__(self, sequence_length, pose_dim, latent_dim, rnn_layer_count, rnn_layer_size, dense_layer_sizes):\n","        super(Encoder, self).__init__()\n","\n","        self.sequence_length = sequence_length\n","        self.pose_dim = pose_dim\n","        self.latent_dim = latent_dim\n","        self.rnn_layer_count = rnn_layer_count\n","        self.rnn_layer_size = rnn_layer_size\n","        self.dense_layer_sizes = dense_layer_sizes\n","\n","        # create recurrent layers\n","        rnn_layers = []\n","        rnn_layers.append((\"encoder_rnn_0\", nn.LSTM(self.pose_dim, self.rnn_layer_size, self.rnn_layer_count, batch_first=True)))\n","\n","        self.rnn_layers = nn.Sequential(OrderedDict(rnn_layers))\n","\n","        # create dense layers\n","\n","        dense_layers = []\n","\n","        dense_layers.append((\"encoder_dense_0\", nn.Linear(self.rnn_layer_size, self.dense_layer_sizes[0])))\n","        dense_layers.append((\"encoder_dense_relu_0\", nn.ReLU()))\n","\n","        dense_layer_count = len(self.dense_layer_sizes)\n","        for layer_index in range(1, dense_layer_count):\n","            dense_layers.append((\"encoder_dense_{}\".format(layer_index), nn.Linear(self.dense_layer_sizes[layer_index-1], self.dense_layer_sizes[layer_index])))\n","            dense_layers.append((\"encoder_dense_relu_{}\".format(layer_index), nn.ReLU()))\n","\n","        self.dense_layers = nn.Sequential(OrderedDict(dense_layers))\n","\n","        # create final dense layers\n","\n","        self.fc_mu = nn.Linear(self.dense_layer_sizes[-1], self.latent_dim)\n","        self.fc_std = nn.Linear(self.dense_layer_sizes[-1], self.latent_dim)\n","\n","    def forward(self, x):\n","\n","        #print(\"x 1 \", x.shape)\n","\n","        x, (_, _) = self.rnn_layers(x)\n","\n","        #print(\"x 2 \", x.shape)\n","\n","        x = x[:, -1, :] # only last time step\n","\n","        #print(\"x 3 \", x.shape)\n","\n","        x = self.dense_layers(x)\n","\n","        #print(\"x 3 \", x.shape)\n","\n","        mu = self.fc_mu(x)\n","        std = self.fc_std(x)\n","\n","        #print(\"mu s \", mu.shape, \" lvar s \", log_var.shape)\n","\n","        return mu, std\n","\n","encoder = Encoder(sequence_length, pose_dim, latent_dim, ae_rnn_layer_count, ae_rnn_layer_size, ae_dense_layer_sizes).to(device)\n","\n","print(encoder)\n","\n","if load_weights and encoder_weights_file:\n","    encoder.load_state_dict(torch.load(encoder_weights_file, map_location=device))\n","\n","# create decoder model\n","\n","class Decoder(nn.Module):\n","    def __init__(self, sequence_length, pose_dim, latent_dim, rnn_layer_count, rnn_layer_size, dense_layer_sizes):\n","        super(Decoder, self).__init__()\n","\n","        self.sequence_length = sequence_length\n","        self.pose_dim = pose_dim\n","        self.latent_dim = latent_dim\n","        self.rnn_layer_size = rnn_layer_size\n","        self.rnn_layer_count = rnn_layer_count\n","        self.dense_layer_sizes = dense_layer_sizes\n","\n","        # create dense layers\n","        dense_layers = []\n","\n","        dense_layers.append((\"decoder_dense_0\", nn.Linear(latent_dim, self.dense_layer_sizes[0])))\n","        dense_layers.append((\"decoder_relu_0\", nn.ReLU()))\n","\n","        dense_layer_count = len(self.dense_layer_sizes)\n","        for layer_index in range(1, dense_layer_count):\n","            dense_layers.append((\"decoder_dense_{}\".format(layer_index), nn.Linear(self.dense_layer_sizes[layer_index-1], self.dense_layer_sizes[layer_index])))\n","            dense_layers.append((\"decoder_dense_relu_{}\".format(layer_index), nn.ReLU()))\n","\n","        self.dense_layers = nn.Sequential(OrderedDict(dense_layers))\n","\n","        # create rnn layers\n","        rnn_layers = []\n","\n","        rnn_layers.append((\"decoder_rnn_0\", nn.LSTM(self.dense_layer_sizes[-1], self.rnn_layer_size, self.rnn_layer_count, batch_first=True)))\n","\n","        self.rnn_layers = nn.Sequential(OrderedDict(rnn_layers))\n","\n","        # final output dense layer\n","        final_layers = []\n","\n","        final_layers.append((\"decoder_dense_{}\".format(dense_layer_count), nn.Linear(self.rnn_layer_size, self.pose_dim)))\n","\n","        self.final_layers = nn.Sequential(OrderedDict(final_layers))\n","\n","    def forward(self, x):\n","        #print(\"x 1 \", x.size())\n","\n","        # dense layers\n","        x = self.dense_layers(x)\n","        #print(\"x 2 \", x.size())\n","\n","        # repeat vector\n","        x = torch.unsqueeze(x, dim=1)\n","        x = x.repeat(1, sequence_length, 1)\n","        #print(\"x 3 \", x.size())\n","\n","        # rnn layers\n","        x, (_, _) = self.rnn_layers(x)\n","        #print(\"x 4 \", x.size())\n","\n","        # final time distributed dense layer\n","        x_reshaped = x.contiguous().view(-1, self.rnn_layer_size)  # (batch_size * sequence, input_size)\n","        #print(\"x 5 \", x_reshaped.size())\n","\n","        yhat = self.final_layers(x_reshaped)\n","        #print(\"yhat 1 \", yhat.size())\n","\n","        yhat = yhat.contiguous().view(-1, self.sequence_length, self.pose_dim)\n","        #print(\"yhat 2 \", yhat.size())\n","\n","        return yhat\n","\n","ae_dense_layer_sizes_reversed = ae_dense_layer_sizes.copy()\n","ae_dense_layer_sizes_reversed.reverse()\n","\n","decoder = Decoder(sequence_length, pose_dim, latent_dim, ae_rnn_layer_count, ae_rnn_layer_size, ae_dense_layer_sizes_reversed).to(device)\n","\n","print(decoder)\n","\n","if load_weights and decoder_weights_file:\n","    decoder.load_state_dict(torch.load(decoder_weights_file, map_location=device))"],"metadata":{"id":"ENvAxcU2uYdc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Forward Kinematics"],"metadata":{"id":"N3RmtoHeufBU"}},{"cell_type":"code","source":["def forward_kinematics(rotations, root_positions):\n","    \"\"\"\n","    Perform forward kinematics using the given trajectory and local rotations.\n","    Arguments (where N = batch size, L = sequence length, J = number of joints):\n","     -- rotations: (N, L, J, 4) tensor of unit quaternions describing the local rotations of each joint.\n","     -- root_positions: (N, L, 3) tensor describing the root joint positions.\n","    \"\"\"\n","\n","    assert len(rotations.shape) == 4\n","    assert rotations.shape[-1] == 4\n","\n","    toffsets = torch.tensor(offsets).to(device)\n","\n","    positions_world = []\n","    rotations_world = []\n","\n","    expanded_offsets = toffsets.expand(rotations.shape[0], rotations.shape[1], offsets.shape[0], offsets.shape[1])\n","\n","    # Parallelize along the batch and time dimensions\n","    for jI in range(offsets.shape[0]):\n","        if parents[jI] == -1:\n","            positions_world.append(root_positions)\n","            rotations_world.append(rotations[:, :, 0])\n","        else:\n","            positions_world.append(qrot(rotations_world[parents[jI]], expanded_offsets[:, :, jI]) \\\n","                                   + positions_world[parents[jI]])\n","            if len(children[jI]) > 0:\n","                rotations_world.append(qmul(rotations_world[parents[jI]], rotations[:, :, jI]))\n","            else:\n","                # This joint is a terminal node -> it would be useless to compute the transformation\n","                rotations_world.append(None)\n","\n","    return torch.stack(positions_world, dim=3).permute(0, 1, 3, 2)"],"metadata":{"id":"-ZP0aQ2buh_b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"sJ4a2OcnunTZ"}},{"cell_type":"code","source":["def calc_kld_scales():\n","\n","    kld_scales = []\n","\n","    for e in range(epochs):\n","\n","        cycle_step = e % kld_scale_cycle_duration\n","\n","        #print(\"cycle_step \", cycle_step)\n","\n","\n","        if cycle_step < kld_scale_min_const_duration:\n","            kld_scale = min_kld_scale\n","            kld_scales.append(kld_scale)\n","        elif cycle_step > kld_scale_cycle_duration - kld_scale_max_const_duration:\n","            kld_scale = max_kld_scale\n","            kld_scales.append(kld_scale)\n","        else:\n","            lin_step = cycle_step - kld_scale_min_const_duration\n","            kld_scale = min_kld_scale + (max_kld_scale - min_kld_scale) * lin_step / (kld_scale_cycle_duration - kld_scale_min_const_duration - kld_scale_max_const_duration)\n","            kld_scales.append(kld_scale)\n","\n","    return kld_scales\n","\n","kld_scales = calc_kld_scales()\n","\n","ae_optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=ae_learning_rate)\n","ae_scheduler = torch.optim.lr_scheduler.StepLR(ae_optimizer, step_size=100, gamma=0.316) # reduce the learning every 100 epochs by a factor of 10\n","\n","mse_loss = nn.MSELoss()\n","cross_entropy = nn.BCELoss()\n","\n","# joint loss weights\n","\n","if len(joint_loss_weights) == 1:\n","    joint_loss_weights *= joint_count\n","\n","joint_loss_weights = torch.tensor(joint_loss_weights, dtype=torch.float32)\n","joint_loss_weights = joint_loss_weights.reshape(1, 1, -1).to(device)\n","\n","# KL Divergence\n","\n","def variational_loss(mu, std):\n","    #returns the varialtional loss from arguments mean and standard deviation std\n","    #see also: see Appendix B from VAE paper:\n","    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n","    #https://arxiv.org/abs/1312.6114\n","    vl=-0.5*torch.mean(1+ 2*torch.log(std)-mu.pow(2) -(std.pow(2)))\n","    return vl\n","\n","def variational_loss2(mu, std):\n","    #returns the varialtional loss from arguments mean and standard deviation std\n","    #alternative: mean squared distance from ideal mu=0 and std=1:\n","    vl=torch.mean(mu.pow(2)+(1-std).pow(2))\n","    return vl\n","\n","def reparameterize(mu, std):\n","    z = mu + std*torch.randn_like(std)\n","    return z\n","\n","def ae_norm_loss(yhat):\n","\n","    _yhat = yhat.view(-1, 4)\n","    _norm = torch.norm(_yhat, dim=1)\n","    _diff = (_norm - 1.0) ** 2\n","    _loss = torch.mean(_diff)\n","    return _loss\n","\n","def forward_kinematics(rotations, root_positions):\n","    \"\"\"\n","    Perform forward kinematics using the given trajectory and local rotations.\n","    Arguments (where N = batch size, L = sequence length, J = number of joints):\n","     -- rotations: (N, L, J, 4) tensor of unit quaternions describing the local rotations of each joint.\n","     -- root_positions: (N, L, 3) tensor describing the root joint positions.\n","    \"\"\"\n","\n","    assert len(rotations.shape) == 4\n","    assert rotations.shape[-1] == 4\n","\n","    toffsets = torch.tensor(offsets).to(device)\n","\n","    positions_world = []\n","    rotations_world = []\n","\n","    expanded_offsets = toffsets.expand(rotations.shape[0], rotations.shape[1], offsets.shape[0], offsets.shape[1])\n","\n","    # Parallelize along the batch and time dimensions\n","    for jI in range(offsets.shape[0]):\n","        if parents[jI] == -1:\n","            positions_world.append(root_positions)\n","            rotations_world.append(rotations[:, :, 0])\n","        else:\n","            positions_world.append(qrot(rotations_world[parents[jI]], expanded_offsets[:, :, jI]) \\\n","                                   + positions_world[parents[jI]])\n","            if len(children[jI]) > 0:\n","                rotations_world.append(qmul(rotations_world[parents[jI]], rotations[:, :, jI]))\n","            else:\n","                # This joint is a terminal node -> it would be useless to compute the transformation\n","                rotations_world.append(None)\n","\n","    return torch.stack(positions_world, dim=3).permute(0, 1, 3, 2)\n","\n","def ae_pos_loss(y, yhat):\n","    # y and yhat shapes: batch_size, seq_length, pose_dim\n","\n","    # normalize tensors\n","    _yhat = yhat.view(-1, 4)\n","\n","    _yhat_norm = nn.functional.normalize(_yhat, p=2, dim=1)\n","    _y_rot = y.view((y.shape[0], y.shape[1], -1, 4))\n","    _yhat_rot = _yhat.view((y.shape[0], y.shape[1], -1, 4))\n","\n","    zero_trajectory = torch.zeros((y.shape[0], y.shape[1], 3), dtype=torch.float32, requires_grad=True).to(device)\n","\n","    _y_pos = forward_kinematics(_y_rot, zero_trajectory)\n","    _yhat_pos = forward_kinematics(_yhat_rot, zero_trajectory)\n","\n","    _pos_diff = torch.norm((_y_pos - _yhat_pos), dim=3)\n","\n","    #print(\"_pos_diff s \", _pos_diff.shape)\n","\n","    _pos_diff_weighted = _pos_diff * joint_loss_weights\n","\n","    _loss = torch.mean(_pos_diff_weighted)\n","\n","    return _loss\n","\n","def ae_quat_loss(y, yhat):\n","    # y and yhat shapes: batch_size, seq_length, pose_dim\n","\n","    # normalize quaternion\n","\n","    _y = y.view((-1, 4))\n","    _yhat = yhat.view((-1, 4))\n","\n","    _yhat_norm = nn.functional.normalize(_yhat, p=2, dim=1)\n","\n","    # inverse of quaternion: https://www.mathworks.com/help/aeroblks/quaternioninverse.html\n","    _yhat_inv = _yhat_norm * torch.tensor([[1.0, -1.0, -1.0, -1.0]], dtype=torch.float32).to(device)\n","\n","    # calculate difference quaternion\n","    _diff = qmul(_yhat_inv, _y)\n","    # length of complex part\n","    _len = torch.norm(_diff[:, 1:], dim=1)\n","    # atan2\n","    _atan = torch.atan2(_len, _diff[:, 0])\n","    # abs\n","    _abs = torch.abs(_atan)\n","\n","    _abs = _abs.reshape(-1, sequence_length, joint_count)\n","\n","    _abs_weighted = _abs * joint_loss_weights\n","\n","    #print(\"_abs s \", _abs.shape)\n","\n","    _loss = torch.mean(_abs_weighted)\n","    return _loss\n","\n","# autoencoder loss function\n","def ae_loss(y, yhat, mu, std):\n","    # function parameters\n","    # y: encoder input\n","    # yhat: decoder output (i.e. reconstructed encoder input)\n","    # disc_fake_output: discriminator output for encoder generated prior\n","\n","    _norm_loss = ae_norm_loss(yhat)\n","    _pos_loss = ae_pos_loss(y, yhat)\n","    _quat_loss = ae_quat_loss(y, yhat)\n","\n","    # kld loss\n","    _ae_kld_loss = variational_loss(mu, std)\n","\n","    _total_loss = 0.0\n","    _total_loss += _norm_loss * ae_norm_loss_scale\n","    _total_loss += _pos_loss * ae_pos_loss_scale\n","    _total_loss += _quat_loss * ae_quat_loss_scale\n","    _total_loss += _ae_kld_loss * ae_kld_loss_scale\n","\n","    return _total_loss, _norm_loss, _pos_loss, _quat_loss, _ae_kld_loss\n","\n","def ae_train_step(target_poses):\n","\n","    #print(\"train step target_poses \", target_poses.shape)\n","\n","    # let autoencoder preproduce target_poses (decoder output) and also return encoder output\n","    encoder_output = encoder(target_poses)\n","\n","    encoder_output_mu = encoder_output[0]\n","    encoder_output_std = encoder_output[1]\n","    mu = torch.tanh(encoder_output_mu)\n","    std = torch.abs(torch.tanh(encoder_output_std)) + 0.00001\n","    decoder_input = reparameterize(mu, std)\n","\n","    pred_poses = decoder(decoder_input)\n","\n","    _ae_loss, _ae_norm_loss, _ae_pos_loss, _ae_quat_loss, _ae_kld_loss = ae_loss(target_poses, pred_poses, mu, std)\n","\n","    #print(\"_ae_pos_loss \", _ae_pos_loss)\n","\n","    # Backpropagation\n","    ae_optimizer.zero_grad()\n","    _ae_loss.backward()\n","\n","    #torch.nn.utils.clip_grad_norm(encoder.parameters(), 0.01)\n","    #torch.nn.utils.clip_grad_norm(decoder.parameters(), 0.01)\n","\n","    ae_optimizer.step()\n","\n","    return _ae_loss, _ae_norm_loss, _ae_pos_loss, _ae_quat_loss, _ae_kld_loss\n","\n","def ae_test_step(target_poses):\n","    with torch.no_grad():\n","        # let autoencoder preproduce target_poses (decoder output) and also return encoder output\n","        encoder_output = encoder(target_poses)\n","\n","        encoder_output_mu = encoder_output[0]\n","        encoder_output_std = encoder_output[1]\n","        mu = torch.tanh(encoder_output_mu)\n","        std = torch.abs(torch.tanh(encoder_output_std)) + 0.00001\n","        decoder_input = reparameterize(mu, std)\n","\n","        pred_poses = decoder(decoder_input)\n","\n","        _ae_loss, _ae_norm_loss, _ae_pos_loss, _ae_quat_loss, _ae_kld_loss = ae_loss(target_poses, pred_poses, mu, std)\n","\n","    return _ae_loss, _ae_norm_loss, _ae_pos_loss, _ae_quat_loss, _ae_kld_loss\n","\n","def train(train_dataloader, test_dataloader, epochs):\n","\n","    global ae_kld_loss_scale\n","\n","    loss_history = {}\n","    loss_history[\"ae train\"] = []\n","    loss_history[\"ae test\"] = []\n","    loss_history[\"ae norm\"] = []\n","    loss_history[\"ae pos\"] = []\n","    loss_history[\"ae quat\"] = []\n","    loss_history[\"ae kld\"] = []\n","\n","    for epoch in range(epochs):\n","\n","        start = time.time()\n","\n","        ae_kld_loss_scale = kld_scales[epoch]\n","\n","        #print(\"ae_kld_loss_scale \", ae_kld_loss_scale)\n","\n","        ae_train_loss_per_epoch = []\n","        ae_norm_loss_per_epoch = []\n","        ae_pos_loss_per_epoch = []\n","        ae_quat_loss_per_epoch = []\n","        ae_prior_loss_per_epoch = []\n","        ae_kld_loss_per_epoch = []\n","\n","        for train_batch in train_dataloader:\n","            train_batch = train_batch.to(device)\n","\n","            _ae_loss, _ae_norm_loss, _ae_pos_loss, _ae_quat_loss, _ae_kld_loss = ae_train_step(train_batch)\n","\n","            _ae_loss = _ae_loss.detach().cpu().numpy()\n","            _ae_norm_loss = _ae_norm_loss.detach().cpu().numpy()\n","            _ae_pos_loss = _ae_pos_loss.detach().cpu().numpy()\n","            _ae_quat_loss = _ae_quat_loss.detach().cpu().numpy()\n","            _ae_kld_loss = _ae_kld_loss.detach().cpu().numpy()\n","\n","            #print(\"_ae_prior_loss \", _ae_prior_loss)\n","\n","            ae_train_loss_per_epoch.append(_ae_loss)\n","            ae_norm_loss_per_epoch.append(_ae_norm_loss)\n","            ae_pos_loss_per_epoch.append(_ae_pos_loss)\n","            ae_quat_loss_per_epoch.append(_ae_quat_loss)\n","            ae_kld_loss_per_epoch.append(_ae_kld_loss)\n","\n","        ae_train_loss_per_epoch = np.mean(np.array(ae_train_loss_per_epoch))\n","        ae_norm_loss_per_epoch = np.mean(np.array(ae_norm_loss_per_epoch))\n","        ae_pos_loss_per_epoch = np.mean(np.array(ae_pos_loss_per_epoch))\n","        ae_quat_loss_per_epoch = np.mean(np.array(ae_quat_loss_per_epoch))\n","        ae_kld_loss_per_epoch = np.mean(np.array(ae_kld_loss_per_epoch))\n","\n","        ae_test_loss_per_epoch = []\n","\n","        for test_batch in test_dataloader:\n","            test_batch = test_batch.to(device)\n","\n","            _ae_loss, _, _, _, _ = ae_test_step(test_batch)\n","\n","            _ae_loss = _ae_loss.detach().cpu().numpy()\n","            ae_test_loss_per_epoch.append(_ae_loss)\n","\n","        ae_test_loss_per_epoch = np.mean(np.array(ae_test_loss_per_epoch))\n","\n","        if epoch % model_save_interval == 0 and save_weights == True:\n","            torch.save(encoder.state_dict(), \"results/weights/encoder_weights_epoch_{}\".format(epoch))\n","            torch.save(decoder.state_dict(), \"results/weights/decoder_weights_epoch_{}\".format(epoch))\n","\n","        loss_history[\"ae train\"].append(ae_train_loss_per_epoch)\n","        loss_history[\"ae test\"].append(ae_test_loss_per_epoch)\n","        loss_history[\"ae norm\"].append(ae_norm_loss_per_epoch)\n","        loss_history[\"ae pos\"].append(ae_pos_loss_per_epoch)\n","        loss_history[\"ae quat\"].append(ae_quat_loss_per_epoch)\n","        loss_history[\"ae kld\"].append(ae_kld_loss_per_epoch)\n","\n","        print ('epoch {} : ae train: {:01.4f} ae test: {:01.4f} norm {:01.4f} pos {:01.4f} quat {:01.4f} kld {:01.4f} time {:01.2f}'.format(epoch + 1, ae_train_loss_per_epoch, ae_test_loss_per_epoch, ae_norm_loss_per_epoch, ae_pos_loss_per_epoch, ae_quat_loss_per_epoch, ae_kld_loss_per_epoch, time.time()-start))\n","\n","        ae_scheduler.step()\n","\n","    return loss_history\n","\n","# fit model\n","loss_history = train(train_dataloader, test_dataloader, epochs)"],"metadata":{"id":"VA_v4QaiupPl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Save Training"],"metadata":{"id":"GQA8BKnxuqAG"}},{"cell_type":"code","source":["# save history\n","utils.save_loss_as_csv(loss_history, \"results/histories/history_{}.csv\".format(epochs))\n","utils.save_loss_as_image(loss_history, \"results/histories/history_{}.png\".format(epochs))\n","\n","# save model weights\n","torch.save(encoder.state_dict(), \"results/weights/encoder_weights_epoch_{}\".format(epochs))\n","torch.save(decoder.state_dict(), \"results/weights/decoder_weights_epoch_{}\".format(epochs))"],"metadata":{"id":"ZAgdcPtZutXm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Inference and Rendering"],"metadata":{"id":"SdlnpzUeuvHK"}},{"cell_type":"code","source":["poseRenderer = PoseRenderer(edge_list)\n","\n","def export_sequence_anim(pose_sequence, file_name):\n","\n","    pose_count = pose_sequence.shape[0]\n","    pose_sequence = np.reshape(pose_sequence, (pose_count, joint_count, joint_dim))\n","\n","    pose_sequence = torch.tensor(np.expand_dims(pose_sequence, axis=0)).to(device)\n","    zero_trajectory = torch.tensor(np.zeros((1, pose_count, 3), dtype=np.float32)).to(device)\n","\n","    skel_sequence = forward_kinematics(pose_sequence, zero_trajectory)\n","\n","    skel_sequence = skel_sequence.detach().cpu().numpy()\n","    skel_sequence = np.squeeze(skel_sequence)\n","\n","    view_min, view_max = utils.get_equal_mix_max_positions(skel_sequence)\n","    skel_images = poseRenderer.create_pose_images(skel_sequence, view_min, view_max, view_ele, view_azi, view_line_width, view_size, view_size)\n","    skel_images[0].save(file_name, save_all=True, append_images=skel_images[1:], optimize=False, duration=33.0, loop=0)\n","\n","def export_sequence_bvh(pose_sequence, file_name):\n","\n","    pose_count = pose_sequence.shape[0]\n","\n","    pred_dataset = {}\n","    pred_dataset[\"frame_rate\"] = mocap_data[\"frame_rate\"]\n","    pred_dataset[\"rot_sequence\"] = mocap_data[\"rot_sequence\"]\n","    pred_dataset[\"skeleton\"] = mocap_data[\"skeleton\"]\n","    pred_dataset[\"motion\"] = {}\n","    pred_dataset[\"motion\"][\"pos_local\"] = np.repeat(np.expand_dims(pred_dataset[\"skeleton\"][\"offsets\"], axis=0), pose_count, axis=0)\n","    pred_dataset[\"motion\"][\"rot_local\"] = pose_sequence\n","    pred_dataset[\"motion\"][\"rot_local_euler\"] = mocap_tools.quat_to_euler_bvh(pred_dataset[\"motion\"][\"rot_local\"], pred_dataset[\"rot_sequence\"])\n","\n","    pred_bvh = mocap_tools.mocap_to_bvh(pred_dataset)\n","\n","    bvh_tools.write(pred_bvh, file_name)\n","\n","def export_sequence_fbx(pose_sequence, file_name):\n","\n","    pose_count = pose_sequence.shape[0]\n","\n","    pred_dataset = {}\n","    pred_dataset[\"frame_rate\"] = mocap_data[\"frame_rate\"]\n","    pred_dataset[\"rot_sequence\"] = mocap_data[\"rot_sequence\"]\n","    pred_dataset[\"skeleton\"] = mocap_data[\"skeleton\"]\n","    pred_dataset[\"motion\"] = {}\n","    pred_dataset[\"motion\"][\"pos_local\"] = np.repeat(np.expand_dims(pred_dataset[\"skeleton\"][\"offsets\"], axis=0), pose_count, axis=0)\n","    pred_dataset[\"motion\"][\"rot_local\"] = pose_sequence\n","    pred_dataset[\"motion\"][\"rot_local_euler\"] = mocap_tools.quat_to_euler(pred_dataset[\"motion\"][\"rot_local\"], pred_dataset[\"rot_sequence\"])\n","\n","    pred_fbx = mocap_tools.mocap_to_fbx([pred_dataset])\n","\n","    fbx_tools.write(pred_fbx, file_name)\n","\n","def encode_sequences(orig_sequence, frame_indices):\n","\n","    encoder.eval()\n","\n","    latent_vectors = []\n","\n","    seq_excerpt_count = len(frame_indices)\n","\n","    for excerpt_index in range(seq_excerpt_count):\n","        excerpt_start_frame = frame_indices[excerpt_index]\n","        excerpt_end_frame = excerpt_start_frame + sequence_length\n","\n","        excerpt = orig_sequence[excerpt_start_frame:excerpt_end_frame]\n","        excerpt = np.expand_dims(excerpt, axis=0)\n","        excerpt = torch.from_numpy(excerpt).reshape(1, sequence_length, pose_dim).to(device)\n","\n","        with torch.no_grad():\n","\n","            encoder_output = encoder(excerpt)\n","\n","            encoder_output_mu = encoder_output[0]\n","            encoder_output_std = encoder_output[1]\n","            mu = torch.tanh(encoder_output_mu)\n","            std = torch.abs(torch.tanh(encoder_output_std)) + 0.00001\n","\n","            latent_vector = reparameterize(mu, std)\n","\n","        latent_vector = torch.squeeze(latent_vector)\n","        latent_vector = latent_vector.detach().cpu().numpy()\n","\n","        latent_vectors.append(latent_vector)\n","\n","    encoder.train()\n","\n","    return latent_vectors\n","\n","def decode_sequence_encodings(sequence_encodings, seq_overlap, base_pose):\n","\n","    decoder.eval()\n","\n","    seq_env = np.hanning(sequence_length)\n","    seq_excerpt_count = len(sequence_encodings)\n","    gen_seq_length = (seq_excerpt_count - 1) * seq_overlap + sequence_length\n","\n","    gen_sequence = np.full(shape=(gen_seq_length, joint_count, joint_dim), fill_value=base_pose)\n","\n","    for excerpt_index in range(len(sequence_encodings)):\n","        latent_vector = sequence_encodings[excerpt_index]\n","        latent_vector = np.expand_dims(latent_vector, axis=0)\n","        latent_vector = torch.from_numpy(latent_vector).to(device)\n","\n","        with torch.no_grad():\n","            excerpt_dec = decoder(latent_vector)\n","\n","        excerpt_dec = torch.squeeze(excerpt_dec)\n","        excerpt_dec = excerpt_dec.detach().cpu().numpy()\n","        excerpt_dec = np.reshape(excerpt_dec, (-1, joint_count, joint_dim))\n","\n","        gen_frame = excerpt_index * seq_overlap\n","\n","        for si in range(sequence_length):\n","            for ji in range(joint_count):\n","                current_quat = gen_sequence[gen_frame + si, ji, :]\n","                target_quat = excerpt_dec[si, ji, :]\n","                quat_mix = seq_env[si]\n","                mix_quat = slerp(current_quat, target_quat, quat_mix )\n","                gen_sequence[gen_frame + si, ji, :] = mix_quat\n","\n","    gen_sequence = gen_sequence.reshape((-1, 4))\n","    gen_sequence = gen_sequence / np.linalg.norm(gen_sequence, ord=2, axis=1, keepdims=True)\n","    gen_sequence = gen_sequence.reshape((gen_seq_length, joint_count, joint_dim))\n","    gen_sequence = qfix(gen_sequence)\n","\n","    decoder.train()\n","\n","    return gen_sequence\n","\n","def create_2d_latent_space_representation(sequence_excerpts):\n","\n","    encodings = []\n","\n","    excerpt_count = sequence_excerpts.shape[0]\n","\n","    for eI in range(0, excerpt_count, batch_size):\n","\n","        excerpt_batch = sequence_excerpts[eI:eI+batch_size]\n","\n","        #print(\"excerpt_batch s \", excerpt_batch.shape)\n","\n","        excerpt_batch = torch.from_numpy(excerpt_batch).to(device)\n","\n","        encoder_output = encoder(excerpt_batch)\n","\n","        encoder_output_mu = encoder_output[0]\n","        encoder_output_std = encoder_output[1]\n","        mu = torch.tanh(encoder_output_mu)\n","        std = torch.abs(torch.tanh(encoder_output_std)) + 0.00001\n","\n","        encoding_batch = reparameterize(mu, std)\n","\n","        #print(\"encoding_batch s \", encoding_batch.shape)\n","\n","        encoding_batch = encoding_batch.detach().cpu()\n","\n","        encodings.append(encoding_batch)\n","\n","    encodings = torch.cat(encodings, dim=0)\n","\n","    #print(\"encodings s \", encodings.shape)\n","\n","    encodings = encodings.numpy()\n","\n","    # use TSNE for dimensionality reduction\n","    tsne = TSNE(n_components=2, n_iter=5000, verbose=1)\n","    Z_tsne = tsne.fit_transform(encodings)\n","\n","    return Z_tsne\n","\n","def create_2d_latent_space_image(Z_tsne, highlight_excerpt_ranges, file_name):\n","\n","    Z_tsne_x = Z_tsne[:,0]\n","    Z_tsne_y = Z_tsne[:,1]\n","\n","    plot_colors = [\"green\", \"red\", \"blue\", \"magenta\", \"orange\"]\n","    plt.figure()\n","    fig, ax = plt.subplots()\n","    ax.plot(Z_tsne_x, Z_tsne_y, '-', c=\"grey\",linewidth=0.2)\n","    ax.scatter(Z_tsne_x, Z_tsne_y, s=0.1, c=\"grey\", alpha=0.5)\n","\n","    for hI, hR in enumerate(highlight_excerpt_ranges):\n","        ax.plot(Z_tsne_x[hR[0]:hR[1]], Z_tsne_y[hR[0]:hR[1]], '-', c=plot_colors[hI],linewidth=0.6)\n","        ax.scatter(Z_tsne_x[hR[0]:hR[1]], Z_tsne_y[hR[0]:hR[1]], s=0.8, c=plot_colors[hI], alpha=0.5)\n","\n","        ax.set_xlabel('$c_1$')\n","        ax.set_ylabel('$c_2$')\n","\n","    fig.savefig(file_name, dpi=300)\n","    plt.close()\n","\n","# create latent space plot\n","\n","Z_tsne = create_2d_latent_space_representation(pose_sequence_excerpts)\n","create_2d_latent_space_image(Z_tsne, [], \"latent_space_plot_epoch_{}.png\".format(epochs))\n","\n","# create original sequence\n","\n","orig_sequence = all_mocap_data[0][\"motion\"][\"rot_local\"].astype(np.float32)\n","\n","seq_start = 7000\n","seq_length = 2000\n","\n","export_sequence_anim(orig_sequence[seq_start:seq_start+seq_length], \"results/anims/orig_sequence_seq_start_{}_length_{}.gif\".format(seq_start, seq_length))\n","export_sequence_fbx(orig_sequence[seq_start:seq_start+seq_length], \"results/anims/orig_sequence_seq_start_{}_length_{}.fbx\".format(seq_start, seq_length))\n","\n","\n","# recontruct original sequence\n","\n","seq_start = 1000\n","seq_length = 1000\n","seq_overlap = 16 # 2 for 8, 32 for 128\n","base_pose = np.reshape(orig_sequence[0], (joint_count, joint_dim))\n","\n","seq_indices = [ frame_index for frame_index in range(seq_start, seq_start + seq_length, seq_overlap)]\n","\n","seq_encodings = encode_sequences(orig_sequence, seq_indices)\n","gen_sequence = decode_sequence_encodings(seq_encodings, seq_overlap, base_pose)\n","export_sequence_anim(gen_sequence, \"results/anims/rec_sequences_epoch_{}_seq_start_{}_length_{}.gif\".format(epochs, seq_start, seq_length))\n","export_sequence_fbx(gen_sequence, \"results/anims/rec_sequences_epoch_{}_seq_start_{}_length_{}.fbx\".format(epochs, seq_start, seq_length))\n","\n","\n","# random walk in latent space\n","seq_start = 1000\n","seq_length = 1000\n","\n","seq_indices = [seq_start]\n","\n","seq_encodings = encode_sequences(orig_sequence, seq_indices)\n","\n","for index in range(0, seq_length // seq_overlap):\n","    random_step = np.random.random((latent_dim)).astype(np.float32) * 2.0\n","    seq_encodings.append(seq_encodings[index] + random_step)\n","\n","gen_sequence = decode_sequence_encodings(seq_encodings, seq_overlap, base_pose)\n","export_sequence_anim(gen_sequence, \"results/anims/seq_randwalk_epoch_{}_seq_start_{}_length_{}.gif\".format(epochs, seq_start, seq_length))\n","export_sequence_fbx(gen_sequence, \"results/anims/seq_randwalk_epoch_{}_seq_start_{}_length_{}.fbx\".format(epochs, seq_start, seq_length))\n","\n","# sequence offset following\n","\n","seq_start = 1000\n","seq_length = 1000\n","\n","seq_indices = [ seq_index for seq_index in range(seq_start, seq_start + seq_length, seq_overlap)]\n","\n","seq_encodings = encode_sequences(orig_sequence, seq_indices)\n","\n","offset_seq_encodings = []\n","\n","for index in range(len(seq_encodings)):\n","    sin_value = np.sin(index / (len(seq_encodings) - 1) * np.pi * 4.0)\n","    offset = np.ones(shape=(latent_dim), dtype=np.float32) * sin_value * 4.0\n","    offset_seq_encoding = seq_encodings[index] + offset\n","    offset_seq_encodings.append(offset_seq_encoding)\n","\n","gen_sequence = decode_sequence_encodings(offset_seq_encodings, seq_overlap, base_pose)\n","export_sequence_anim(gen_sequence, \"results/anims/seq_offset_epoch_{}_seq_start_{}_length_{}.gif\".format(epochs, seq_start, seq_length))\n","export_sequence_fbx(gen_sequence, \"results/anims/seq_offset_epoch_{}_seq_start_{}_length_{}.fbx\".format(epochs, seq_start, seq_length))\n","\n","\n","\n","# interpolate two original sequences\n","\n","seq1_start = 1000\n","seq2_start = 2000\n","seq_length = 1000\n","\n","seq1_indices = [ seq_index for seq_index in range(seq1_start, seq1_start + seq_length, seq_overlap)]\n","seq2_indices = [ seq_index for seq_index in range(seq2_start, seq2_start + seq_length, seq_overlap)]\n","\n","seq1_encodings = encode_sequences(orig_sequence, seq1_indices)\n","seq2_encodings = encode_sequences(orig_sequence, seq2_indices)\n","\n","mix_encodings = []\n","\n","for index in range(len(seq1_encodings)):\n","    mix_factor = index / (len(seq1_indices) - 1)\n","    mix_encoding = seq1_encodings[index] * (1.0 - mix_factor) + seq2_encodings[index] * mix_factor\n","    mix_encodings.append(mix_encoding)\n","\n","gen_sequence = decode_sequence_encodings(mix_encodings, seq_overlap, base_pose)\n","export_sequence_anim(gen_sequence, \"results/anims/seq_mix_epoch_{}_seq1_start_{}_seq2_start_{}_length_{}.gif\".format(epochs, seq1_start, seq2_start, seq_length))\n","export_sequence_fbx(gen_sequence, \"results/anims/seq_mix_epoch_{}_seq1_start_{}_seq2_start_{}_length_{}.fbx\".format(epochs, seq1_start, seq2_start, seq_length))"],"metadata":{"id":"f-AKnBrnuyRt"},"execution_count":null,"outputs":[]}]}