{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyM1kTrHOGb+EEeQ+rsW6puN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Drive"],"metadata":{"id":"I4HC_XXl7FHI"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UY7mEOq-6560"},"outputs":[],"source":["import sys\n","from google.colab import drive\n","\n","# mount google drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# insert directory\n","sys.path.insert(0, '/content/drive/MyDrive/CAS_AIArt/AIMovement/MotionTransformation/VAE-RNN')"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/CAS_AIArt/AIMovement/MotionTransformation/VAE-RNN/\n","%ls\n","%pwd"],"metadata":{"id":"HeIe-dny7KVx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Installation"],"metadata":{"id":"U0kkrq0W7M07"}},{"cell_type":"code","source":["!pip install transforms3d\n","!pip install scipy==1.14.1\n","!pip install installers/fbx-2020.3.7-cp310-cp310-manylinux1_x86_64.whl"],"metadata":{"id":"N7Csu_hn7HV9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"CrlGZNw47PXs"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torch import nn\n","from collections import OrderedDict\n","\n","import os, sys, time, subprocess\n","import numpy as np\n","import json\n","import pickle\n","\n","from common import utils\n","from common import mocap_tools as mocap\n","from common.quaternion import qmul, qrot, qnormalize_np, slerp, qfix\n","from common.pose_renderer import PoseRenderer\n","\n","from sklearn.manifold import TSNE\n","from matplotlib import pyplot as plt"],"metadata":{"id":"pCGrBtvz7Re7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Device"],"metadata":{"id":"5-2FhGMT7UF6"}},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print('Using {} device'.format(device))"],"metadata":{"id":"QzuBsTVb7WiP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Mocap Settings"],"metadata":{"id":"MFV8nDrv7X_h"}},{"cell_type":"code","source":["mocap_config_file = \"configs/Halpe26_config.json\"\n","mocap_file_path = \"data/\"\n","mocap_files = [\"HannahMartin_Pos2D_Performance.pkl\"]\n","mocap_valid_frame_ranges = [ [ [ 0, 3600 ] ] ]\n","mocap_sensor_ids = [\"/mocap/0/joint/pos2d_world\", \"/mocap/0/joint/visibility\"]\n","mocap_root_joint_name = \"Hip\"\n","mocap_fps = 30\n","mocap_joint_dim = 2"],"metadata":{"id":"ZFQyl19F7bZR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model Settings"],"metadata":{"id":"M7hjsL1h7hey"}},{"cell_type":"code","source":["latent_dim = 32\n","sequence_length = 64\n","ae_rnn_layer_count = 2\n","ae_rnn_layer_size = 512\n","ae_dense_layer_sizes = [ 512 ]\n","\n","save_models = False\n","save_tscript = False\n","save_weights = True\n","\n","# load model weights\n","load_weights = False\n","encoder_weights_file = \"results/weights/encoder_weights_epoch_600\"\n","decoder_weights_file = \"results/weights/decoder_weights_epoch_600\""],"metadata":{"id":"vEoKocFD7jTR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training Settings"],"metadata":{"id":"mLdHjauq7oil"}},{"cell_type":"code","source":["sequence_offset = 2 # when creating sequence excerpts, each excerpt is offset from the previous one by this value\n","batch_size = 16\n","train_percentage = 0.8 # train / test split\n","test_percentage  = 0.2\n","ae_learning_rate = 1e-4\n","ae_pos_loss_scale = 1.0\n","ae_kld_loss_scale = 0.0 # will be calculated\n","kld_scale_cycle_duration = 100\n","kld_scale_min_const_duration = 20\n","kld_scale_max_const_duration = 20\n","min_kld_scale = 0.0\n","max_kld_scale = 0.1\n","\n","epochs = 200\n","model_save_interval = 50\n","save_history = True\n","joint_loss_weights = [1.0]"],"metadata":{"id":"EMe4ynjC7p1h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Visualization Settings"],"metadata":{"id":"Ybos8PX37ta3"}},{"cell_type":"code","source":["if mocap_joint_dim == 2:\n","    view_ele = 90.0\n","    view_azi = 90.0\n","    view_line_width = 1.0\n","    view_size = 4.0\n","else:\n","    view_ele = 0.0\n","    view_azi = 0.0\n","    view_line_width = 1.0\n","    view_size = 4.0"],"metadata":{"id":"bvlkVPKU7unQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load Mocap Data"],"metadata":{"id":"DniM24RW7was"}},{"cell_type":"code","source":["with open(mocap_config_file) as f:\n","    mocap_config = json.load(f)\n","\n","\n","def config_to_skeletondata(mocap_config):\n","\n","    skeleton_data = {}\n","    skeleton_data[\"joints\"] = mocap_config[\"jointNames\"]\n","    skeleton_data[\"root\"] = skeleton_data[\"joints\"][0]\n","    skeleton_data[\"parents\"] = mocap_config[\"jointParents\"]\n","    skeleton_data[\"children\"] = mocap_config[\"jointChildren\"]\n","\n","    return skeleton_data\n","\n","def recording_to_motiondata(mocap_recording, skeleton_data, mocap_sensor_ids):\n","\n","    joint_count = len(skeleton_data[\"joints\"])\n","\n","    # gather sensor values\n","    motion_data = {}\n","\n","    sensor_ids = mocap_recording[\"sensor_ids\"]\n","    sensor_values = mocap_recording[\"sensor_values\"]\n","\n","    for sensor_id in mocap_sensor_ids:\n","\n","        #print(\"sensor_id \", sensor_id)\n","        motion_data[sensor_id]  = [ sensor_values [vI] for vI in range(len(sensor_values)) if sensor_ids[vI].endswith(sensor_id) ]\n","        motion_data[sensor_id] = np.array(motion_data[sensor_id], dtype=np.float32)\n","        motion_data[sensor_id] = np.reshape(motion_data[sensor_id], (motion_data[sensor_id].shape[0], joint_count, -1))\n","\n","    return motion_data\n","\n","skeleton_data = config_to_skeletondata(mocap_config)\n","\n","all_motion_data = []\n","\n","for mocap_file in mocap_files:\n","\n","    print(\"process file \", mocap_file)\n","\n","    with open(mocap_file_path + \"/\" + mocap_file, \"rb\") as f:\n","        mocap_recording = pickle.load(f)\n","\n","        motion_data = recording_to_motiondata(mocap_recording, skeleton_data, mocap_sensor_ids)\n","\n","        all_motion_data.append(motion_data)\n","\n","# retrieve mocap properties\n","\n","joint_count = len(skeleton_data[\"joints\"])\n","joint_dim = mocap_joint_dim\n","pose_dim = joint_count * joint_dim\n","\n","parents = skeleton_data[\"parents\"]\n","children = skeleton_data[\"children\"]\n","\n","joint_loss_weights = joint_loss_weights * joint_count\n","\n","# create edge list\n","def get_edge_list(children):\n","    edge_list = []\n","\n","    for parent_joint_index in range(len(children)):\n","        for child_joint_index in children[parent_joint_index]:\n","            edge_list.append([parent_joint_index, child_joint_index])\n","\n","    return edge_list\n","\n","edge_list = get_edge_list(children)\n","\n","# set root position to zero\n","mocap_root_joint_index = skeleton_data[\"joints\"].index(mocap_root_joint_name)\n","\n","for motion_data in all_motion_data:\n","\n","    if joint_dim == 3:\n","        joint_pos = motion_data[\"/mocap/0/joint/pos3d_world\"]\n","    else:\n","        joint_pos = motion_data[\"/mocap/0/joint/pos2d_world\"]\n","\n","    root_pos = joint_pos[:, mocap_root_joint_index:mocap_root_joint_index+1, :]\n","\n","    joint_pos_root_zero = joint_pos - root_pos\n","\n","    motion_data[\"/mocap/0/joint/pos_root_zero\"] = joint_pos_root_zero\n","\n","# calculate pose normalisation values\n","pose_sequence_all = []\n","for motion_data in all_motion_data:\n","    pose_sequence = motion_data[\"/mocap/0/joint/pos_root_zero\"]\n","    pose_sequence_all.append(pose_sequence)\n","\n","pose_sequence_all = np.concatenate(pose_sequence_all, axis=0)\n","#pose_sequence_all = np.reshape(pose_sequence_all, (-1, pose_dim))\n","\n","pose_mean = np.mean(pose_sequence_all, axis=0).flatten()\n","pose_std = np.std(pose_sequence_all, axis=0).flatten()"],"metadata":{"id":"V28GAQbf7yVg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create Dataset"],"metadata":{"id":"eupYIIx972vz"}},{"cell_type":"code","source":["pose_sequence_excerpts = []\n","vis_sequence_excerpts = []\n","\n","for motion_data in all_motion_data:\n","\n","    pose_sequence = motion_data[\"/mocap/0/joint/pos_root_zero\"]\n","    pose_sequence = np.reshape(pose_sequence, (-1, pose_dim))\n","\n","    #print(\"pose_sequence s \", pose_sequence.shape)\n","\n","    vis_sequence = motion_data[\"/mocap/0/joint/visibility\"]\n","    vis_sequence = np.reshape(vis_sequence, (-1, joint_count))\n","\n","    #print(\"vis_sequence s \", vis_sequence.shape)\n","\n","    frame_range_start = 0\n","    frame_range_end = pose_sequence.shape[0]\n","\n","    for seq_excerpt_start in np.arange(frame_range_start, frame_range_end - sequence_length, sequence_offset):\n","        #print(\"valid: start \", frame_range_start, \" end \", frame_range_end, \" exc: start \", seq_excerpt_start, \" end \", (seq_excerpt_start + sequence_length) )\n","        pose_sequence_excerpt =  pose_sequence[seq_excerpt_start:seq_excerpt_start + sequence_length]\n","        pose_sequence_excerpts.append(pose_sequence_excerpt)\n","\n","        #print(\"pose_sequence_excerpt s \", pose_sequence_excerpt.shape)\n","\n","        vis_sequence_excerpt = vis_sequence[seq_excerpt_start:seq_excerpt_start + sequence_length]\n","\n","        #print(\"vis_sequence_excerpt s \", vis_sequence_excerpt.shape)\n","\n","        vis_sequence_excerpts.append(vis_sequence_excerpt)\n","\n","\n","pose_sequence_excerpts = np.array(pose_sequence_excerpts, dtype=np.float32)\n","vis_sequence_excerpts = np.array(vis_sequence_excerpts, dtype=np.float32)\n","\n","# create dataset\n","\n","sequence_excerpts_count = pose_sequence_excerpts.shape[0]\n","\n","class SequenceDataset(Dataset):\n","    def __init__(self, pose_sequence_excerpts, vis_sequence_excerpts):\n","        self.pose_sequence_excerpts = pose_sequence_excerpts\n","        self.vis_sequence_excerpts = vis_sequence_excerpts\n","\n","    def __len__(self):\n","        return self.pose_sequence_excerpts.shape[0]\n","\n","    def __getitem__(self, idx):\n","        return self.pose_sequence_excerpts[idx, ...], self.vis_sequence_excerpts[idx, ...]\n","\n","\n","full_dataset = SequenceDataset(pose_sequence_excerpts, vis_sequence_excerpts)\n","dataset_size = len(full_dataset)\n","\n","pose_seq_item, vis_seq_item = full_dataset[0]\n","print(\"pose_seq_item s \", pose_seq_item.shape)\n","print(\"vis_seq_item s \", vis_seq_item.shape)\n","\n","test_size = int(test_percentage * dataset_size)\n","train_size = dataset_size - test_size\n","\n","train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","pose_seq_batch, vis_seq_batch = next(iter(train_dataloader))\n","\n","print(\"pose_seq_batch s \", pose_seq_batch.shape)\n","print(\"vis_seq_batch s \", vis_seq_batch.shape)"],"metadata":{"id":"LDGn1V2R73k6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create Models"],"metadata":{"id":"kzpJ8Bh_755Y"}},{"cell_type":"code","source":["# create encoder model\n","\n","class Encoder(nn.Module):\n","    def __init__(self, sequence_length, pose_dim, latent_dim, rnn_layer_count, rnn_layer_size, dense_layer_sizes):\n","        super(Encoder, self).__init__()\n","\n","        self.sequence_length = sequence_length\n","        self.pose_dim = pose_dim\n","        self.latent_dim = latent_dim\n","        self.rnn_layer_count = rnn_layer_count\n","        self.rnn_layer_size = rnn_layer_size\n","        self.dense_layer_sizes = dense_layer_sizes\n","\n","        # create recurrent layers\n","        rnn_layers = []\n","        rnn_layers.append((\"encoder_rnn_0\", nn.LSTM(self.pose_dim, self.rnn_layer_size, self.rnn_layer_count, batch_first=True)))\n","\n","        self.rnn_layers = nn.Sequential(OrderedDict(rnn_layers))\n","\n","        # create dense layers\n","\n","        dense_layers = []\n","\n","        dense_layers.append((\"encoder_dense_0\", nn.Linear(self.rnn_layer_size, self.dense_layer_sizes[0])))\n","        dense_layers.append((\"encoder_dense_relu_0\", nn.ReLU()))\n","\n","        dense_layer_count = len(self.dense_layer_sizes)\n","        for layer_index in range(1, dense_layer_count):\n","            dense_layers.append((\"encoder_dense_{}\".format(layer_index), nn.Linear(self.dense_layer_sizes[layer_index-1], self.dense_layer_sizes[layer_index])))\n","            dense_layers.append((\"encoder_dense_relu_{}\".format(layer_index), nn.ReLU()))\n","\n","        self.dense_layers = nn.Sequential(OrderedDict(dense_layers))\n","\n","        # create final dense layers\n","\n","        self.fc_mu = nn.Linear(self.dense_layer_sizes[-1], self.latent_dim)\n","        self.fc_std = nn.Linear(self.dense_layer_sizes[-1], self.latent_dim)\n","\n","    def forward(self, x):\n","\n","        #print(\"x 1 \", x.shape)\n","\n","        x, (_, _) = self.rnn_layers(x)\n","\n","        #print(\"x 2 \", x.shape)\n","\n","        x = x[:, -1, :] # only last time step\n","\n","        #print(\"x 3 \", x.shape)\n","\n","        x = self.dense_layers(x)\n","\n","        #print(\"x 3 \", x.shape)\n","\n","        mu = self.fc_mu(x)\n","        std = self.fc_std(x)\n","\n","        #print(\"mu s \", mu.shape, \" lvar s \", log_var.shape)\n","\n","        return mu, std\n","\n","encoder = Encoder(sequence_length, pose_dim, latent_dim, ae_rnn_layer_count, ae_rnn_layer_size, ae_dense_layer_sizes).to(device)\n","\n","print(encoder)\n","\n","if load_weights and encoder_weights_file:\n","    encoder.load_state_dict(torch.load(encoder_weights_file, map_location=device))\n","\n","# create decoder model\n","\n","class Decoder(nn.Module):\n","    def __init__(self, sequence_length, pose_dim, latent_dim, rnn_layer_count, rnn_layer_size, dense_layer_sizes):\n","        super(Decoder, self).__init__()\n","\n","        self.sequence_length = sequence_length\n","        self.pose_dim = pose_dim\n","        self.latent_dim = latent_dim\n","        self.rnn_layer_size = rnn_layer_size\n","        self.rnn_layer_count = rnn_layer_count\n","        self.dense_layer_sizes = dense_layer_sizes\n","\n","        # create dense layers\n","        dense_layers = []\n","\n","        dense_layers.append((\"decoder_dense_0\", nn.Linear(latent_dim, self.dense_layer_sizes[0])))\n","        dense_layers.append((\"decoder_relu_0\", nn.ReLU()))\n","\n","        dense_layer_count = len(self.dense_layer_sizes)\n","        for layer_index in range(1, dense_layer_count):\n","            dense_layers.append((\"decoder_dense_{}\".format(layer_index), nn.Linear(self.dense_layer_sizes[layer_index-1], self.dense_layer_sizes[layer_index])))\n","            dense_layers.append((\"decoder_dense_relu_{}\".format(layer_index), nn.ReLU()))\n","\n","        self.dense_layers = nn.Sequential(OrderedDict(dense_layers))\n","\n","        # create rnn layers\n","        rnn_layers = []\n","\n","        rnn_layers.append((\"decoder_rnn_0\", nn.LSTM(self.dense_layer_sizes[-1], self.rnn_layer_size, self.rnn_layer_count, batch_first=True)))\n","\n","        self.rnn_layers = nn.Sequential(OrderedDict(rnn_layers))\n","\n","        # final output dense layer\n","        final_layers = []\n","\n","        final_layers.append((\"decoder_dense_{}\".format(dense_layer_count), nn.Linear(self.rnn_layer_size, self.pose_dim)))\n","\n","        self.final_layers = nn.Sequential(OrderedDict(final_layers))\n","\n","    def forward(self, x):\n","        #print(\"x 1 \", x.size())\n","\n","        # dense layers\n","        x = self.dense_layers(x)\n","        #print(\"x 2 \", x.size())\n","\n","        # repeat vector\n","        x = torch.unsqueeze(x, dim=1)\n","        x = x.repeat(1, sequence_length, 1)\n","        #print(\"x 3 \", x.size())\n","\n","        # rnn layers\n","        x, (_, _) = self.rnn_layers(x)\n","        #print(\"x 4 \", x.size())\n","\n","        # final time distributed dense layer\n","        x_reshaped = x.contiguous().view(-1, self.rnn_layer_size)  # (batch_size * sequence, input_size)\n","        #print(\"x 5 \", x_reshaped.size())\n","\n","        yhat = self.final_layers(x_reshaped)\n","        #print(\"yhat 1 \", yhat.size())\n","\n","        yhat = yhat.contiguous().view(-1, self.sequence_length, self.pose_dim)\n","        #print(\"yhat 2 \", yhat.size())\n","\n","        return yhat\n","\n","ae_dense_layer_sizes_reversed = ae_dense_layer_sizes.copy()\n","ae_dense_layer_sizes_reversed.reverse()\n","\n","decoder = Decoder(sequence_length, pose_dim, latent_dim, ae_rnn_layer_count, ae_rnn_layer_size, ae_dense_layer_sizes_reversed).to(device)\n","\n","print(decoder)\n","\n","if load_weights and decoder_weights_file:\n","    decoder.load_state_dict(torch.load(decoder_weights_file, map_location=device))"],"metadata":{"id":"rqRp7Xz8765H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"lVGD1zTh7_dL"}},{"cell_type":"code","source":["def calc_kld_scales():\n","\n","    kld_scales = []\n","\n","    for e in range(epochs):\n","\n","        cycle_step = e % kld_scale_cycle_duration\n","\n","        #print(\"cycle_step \", cycle_step)\n","\n","\n","        if cycle_step < kld_scale_min_const_duration:\n","            kld_scale = min_kld_scale\n","            kld_scales.append(kld_scale)\n","        elif cycle_step > kld_scale_cycle_duration - kld_scale_max_const_duration:\n","            kld_scale = max_kld_scale\n","            kld_scales.append(kld_scale)\n","        else:\n","            lin_step = cycle_step - kld_scale_min_const_duration\n","            kld_scale = min_kld_scale + (max_kld_scale - min_kld_scale) * lin_step / (kld_scale_cycle_duration - kld_scale_min_const_duration - kld_scale_max_const_duration)\n","            kld_scales.append(kld_scale)\n","\n","    return kld_scales\n","\n","kld_scales = calc_kld_scales()\n","\n","ae_optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=ae_learning_rate)\n","ae_scheduler = torch.optim.lr_scheduler.StepLR(ae_optimizer, step_size=50, gamma=0.33) # reduce the learning every 100 epochs by a factor of 10\n","\n","mse_loss = nn.MSELoss()\n","cross_entropy = nn.BCELoss()\n","\n","# pose mean and std\n","\n","pose_mean = torch.tensor(pose_mean).reshape(1, 1, -1).to(device)\n","pose_std = torch.tensor(pose_std).reshape(1, 1, -1).to(device)\n","\n","# joint loss weights\n","\n","if len(joint_loss_weights) == 1:\n","    joint_loss_weights *= joint_count\n","\n","joint_loss_weights = torch.tensor(joint_loss_weights, dtype=torch.float32)\n","joint_loss_weights = joint_loss_weights.reshape(1, 1, -1).to(device)\n","\n","# KL Divergence\n","\n","def variational_loss(mu, std):\n","    #returns the varialtional loss from arguments mean and standard deviation std\n","    #see also: see Appendix B from VAE paper:\n","    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n","    #https://arxiv.org/abs/1312.6114\n","    vl=-0.5*torch.mean(1+ 2*torch.log(std)-mu.pow(2) -(std.pow(2)))\n","    return vl\n","\n","def variational_loss2(mu, std):\n","    #returns the varialtional loss from arguments mean and standard deviation std\n","    #alternative: mean squared distance from ideal mu=0 and std=1:\n","    vl=torch.mean(mu.pow(2)+(1-std).pow(2))\n","    return vl\n","\n","def reparameterize(mu, std):\n","    z = mu + std*torch.randn_like(std)\n","    return z\n","\n","def ae_pos_loss(y, yhat, vis):\n","    # y and yhat shapes: batch_size, seq_length, pose_dim\n","\n","    #print(\"ae_pos_loss\")\n","    #print(\"y s \", y.shape)\n","    #print(\"yhat s \", yhat.shape)\n","    #print(\"vis s \", vis.shape)\n","\n","    # reshape into batch_size x sequence_length x joint_count x joint_dim\n","    _y = y.reshape(y.shape[0], y.shape[1], joint_count, joint_dim )\n","    _yhat = yhat.reshape(y.shape[0], y.shape[1], joint_count, joint_dim )\n","\n","    #print(\"_y s \", _y.shape)\n","    #print(\"_yhat s \", _yhat.shape)\n","\n","    _pos_diff = torch.norm((_y - _yhat), dim=3)\n","\n","    #print(\"_pos_diff s \", _pos_diff.shape)\n","\n","    #print(\"_pos_diff s \", _pos_diff.shape)\n","    #print(\"joint_loss_weights s \", joint_loss_weights.shape)\n","    #print(\"_vis s \", _vis.shape)\n","\n","    _pos_diff_weighted = _pos_diff * joint_loss_weights * vis\n","\n","    #_pos_diff_weighted = _pos_diff * joint_loss_weights.reshape(1, joint_count) * _vis\n","\n","    _loss = torch.mean(_pos_diff_weighted)\n","\n","    return _loss\n","\n","# autoencoder loss function\n","def ae_loss(y, yhat, vis, mu, std):\n","    # function parameters\n","    # y: encoder input\n","    # yhat: decoder output (i.e. reconstructed encoder input)\n","    # disc_fake_output: discriminator output for encoder generated prior\n","\n","    _pos_loss = ae_pos_loss(y, yhat, vis)\n","\n","    # kld loss\n","    _ae_kld_loss = variational_loss(mu, std)\n","\n","    _total_loss = 0.0\n","    _total_loss += _pos_loss * ae_pos_loss_scale\n","    _total_loss += _ae_kld_loss * ae_kld_loss_scale\n","\n","    return _total_loss, _pos_loss, _ae_kld_loss\n","\n","def ae_train_step(target_poses, joint_vis):\n","\n","    # normalise target poses\n","    target_poses_norm = (target_poses - pose_mean) / pose_std\n","    target_poses_norm = torch.nan_to_num(target_poses_norm)\n","\n","    # let autoencoder preproduce target_poses (decoder output) and also return encoder output\n","    encoder_output = encoder(target_poses_norm)\n","\n","    encoder_output_mu = encoder_output[0]\n","    encoder_output_std = encoder_output[1]\n","    mu = torch.tanh(encoder_output_mu)\n","    std = torch.abs(torch.tanh(encoder_output_std)) + 0.00001\n","    decoder_input = reparameterize(mu, std)\n","\n","    pred_poses_norm = decoder(decoder_input)\n","\n","    _ae_loss, _ae_pos_loss, _ae_kld_loss = ae_loss(target_poses_norm, pred_poses_norm, joint_vis, mu, std)\n","\n","    #print(\"_ae_pos_loss \", _ae_pos_loss)\n","\n","    # Backpropagation\n","    ae_optimizer.zero_grad()\n","    _ae_loss.backward()\n","\n","    #torch.nn.utils.clip_grad_norm(encoder.parameters(), 0.01)\n","    #torch.nn.utils.clip_grad_norm(decoder.parameters(), 0.01)\n","\n","    ae_optimizer.step()\n","\n","    return _ae_loss, _ae_pos_loss, _ae_kld_loss\n","\n","def ae_test_step(target_poses, joint_vis):\n","\n","    with torch.no_grad():\n","        # normalise target poses\n","        target_poses_norm = (target_poses - pose_mean) / pose_std\n","        target_poses_norm = torch.nan_to_num(target_poses_norm)\n","\n","        # let autoencoder preproduce target_poses (decoder output) and also return encoder output\n","        encoder_output = encoder(target_poses_norm)\n","\n","        encoder_output_mu = encoder_output[0]\n","        encoder_output_std = encoder_output[1]\n","        mu = torch.tanh(encoder_output_mu)\n","        std = torch.abs(torch.tanh(encoder_output_std)) + 0.00001\n","        decoder_input = reparameterize(mu, std)\n","\n","        pred_poses_norm = decoder(decoder_input)\n","\n","        _ae_loss, _ae_pos_loss, _ae_kld_loss = ae_loss(target_poses_norm, pred_poses_norm, joint_vis, mu, std)\n","\n","        #print(\"_ae_pos_loss \", _ae_pos_loss)\n","\n","    return _ae_loss, _ae_pos_loss, _ae_kld_loss\n","\n","\n","def train(train_dataloader, test_dataloader, epochs):\n","\n","    global ae_kld_loss_scale\n","\n","    loss_history = {}\n","    loss_history[\"ae train\"] = []\n","    loss_history[\"ae test\"] = []\n","    loss_history[\"ae pos\"] = []\n","    loss_history[\"ae kld\"] = []\n","\n","    for epoch in range(epochs):\n","\n","        start = time.time()\n","\n","        ae_kld_loss_scale = kld_scales[epoch]\n","\n","        #print(\"ae_kld_loss_scale \", ae_kld_loss_scale)\n","\n","        ae_train_loss_per_epoch = []\n","        ae_pos_loss_per_epoch = []\n","        ae_prior_loss_per_epoch = []\n","        ae_kld_loss_per_epoch = []\n","\n","        for train_batch_pose, train_batch_vis in train_dataloader:\n","            train_batch_pose = train_batch_pose.to(device)\n","            train_batch_vis = train_batch_vis.to(device)\n","\n","            _ae_loss, _ae_pos_loss, _ae_kld_loss = ae_train_step(train_batch_pose, train_batch_vis)\n","\n","            _ae_loss = _ae_loss.detach().cpu().numpy()\n","            _ae_pos_loss = _ae_pos_loss.detach().cpu().numpy()\n","            _ae_kld_loss = _ae_kld_loss.detach().cpu().numpy()\n","\n","            #print(\"_ae_prior_loss \", _ae_prior_loss)\n","\n","            ae_train_loss_per_epoch.append(_ae_loss)\n","            ae_pos_loss_per_epoch.append(_ae_pos_loss)\n","            ae_kld_loss_per_epoch.append(_ae_kld_loss)\n","\n","        ae_train_loss_per_epoch = np.mean(np.array(ae_train_loss_per_epoch))\n","        ae_pos_loss_per_epoch = np.mean(np.array(ae_pos_loss_per_epoch))\n","        ae_kld_loss_per_epoch = np.mean(np.array(ae_kld_loss_per_epoch))\n","\n","        ae_test_loss_per_epoch = []\n","\n","        for test_batch_pose, test_batch_vis in test_dataloader:\n","            test_batch_pose = test_batch_pose.to(device)\n","            test_batch_vis = test_batch_vis.to(device)\n","\n","            _ae_loss, _, _ = ae_test_step(test_batch_pose, test_batch_vis)\n","\n","            _ae_loss = _ae_loss.detach().cpu().numpy()\n","            ae_test_loss_per_epoch.append(_ae_loss)\n","\n","        ae_test_loss_per_epoch = np.mean(np.array(ae_test_loss_per_epoch))\n","\n","        if epoch % model_save_interval == 0 and save_weights == True:\n","            torch.save(encoder.state_dict(), \"results/weights/encoder_weights_epoch_{}\".format(epoch))\n","            torch.save(decoder.state_dict(), \"results/weights/decoder_weights_epoch_{}\".format(epoch))\n","\n","        loss_history[\"ae train\"].append(ae_train_loss_per_epoch)\n","        loss_history[\"ae test\"].append(ae_test_loss_per_epoch)\n","        loss_history[\"ae pos\"].append(ae_pos_loss_per_epoch)\n","        loss_history[\"ae kld\"].append(ae_kld_loss_per_epoch)\n","\n","        print ('epoch {} : ae train: {:01.4f} ae test: {:01.4f} pos {:01.4f} kld {:01.4f} time {:01.2f}'.format(epoch + 1, ae_train_loss_per_epoch, ae_test_loss_per_epoch, ae_pos_loss_per_epoch, ae_kld_loss_per_epoch, time.time()-start))\n","\n","        ae_scheduler.step()\n","\n","    return loss_history\n","\n","# fit model\n","loss_history = train(train_dataloader, test_dataloader, epochs)"],"metadata":{"id":"vgxhDW8e8As1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Save Training"],"metadata":{"id":"Vyg_Im158HVS"}},{"cell_type":"code","source":["# save history\n","utils.save_loss_as_csv(loss_history, \"results/histories/history_{}.csv\".format(epochs))\n","utils.save_loss_as_image(loss_history, \"results/histories/history_{}.png\".format(epochs))\n","\n","# save model weights\n","torch.save(encoder.state_dict(), \"results/weights/encoder_weights_epoch_{}\".format(epochs))\n","torch.save(decoder.state_dict(), \"results/weights/decoder_weights_epoch_{}\".format(epochs))"],"metadata":{"id":"bTwo6lzw8IOe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Inference and Rendering"],"metadata":{"id":"hH2coMSH8Jqf"}},{"cell_type":"code","source":["poseRenderer = PoseRenderer(edge_list)\n","\n","def export_sequence_anim(pose_sequence, file_name):\n","\n","    pose_count = pose_sequence.shape[0]\n","    pose_sequence = np.reshape(pose_sequence, (pose_count, joint_count, joint_dim))\n","\n","    if joint_dim == 2: # add third axis to pose sequence\n","        pose_sequence = np.concatenate([pose_sequence, np.zeros((pose_count, joint_count, 1))], axis=2)\n","\n","    skel_sequence = pose_sequence\n","\n","    view_min, view_max = utils.get_equal_mix_max_positions(skel_sequence)\n","    skel_images = poseRenderer.create_pose_images(skel_sequence, view_min, view_max, view_ele, view_azi, view_line_width, view_size, view_size)\n","    skel_images[0].save(file_name, save_all=True, append_images=skel_images[1:], optimize=False, duration=33.0, loop=0)\n","\n","def export_sequence_pkl(pose_sequence, file_name):\n","\n","    pose_count = pose_sequence.shape[0]\n","    pose_sequence = np.reshape(pose_sequence, (pose_count, pose_dim))\n","\n","\n","    export_dict = {}\n","    export_dict[\"class_id\"] = 0\n","    if joint_dim == 2:\n","        export_dict[\"sensor_ids\"] = [\"/mocap/0/joint/pos2d_world\"] * pose_count\n","    else:\n","        export_dict[\"sensor_ids\"] = [\"/mocap/0/joint/pos3d_world\"] * pose_count\n","\n","    # this should be a list of tuples, but I guess a list of lists is also fine\n","    export_dict[\"sensor_values\"] = pose_sequence.tolist()\n","    export_dict[\"time_stamps\"] = (np.arange(0, pose_count, 1) * (1.0 / mocap_fps)).tolist()\n","\n","    with open(file_name, \"wb\") as f:\n","        pickle.dump(export_dict, f)\n","\n","def encode_sequences(pose_sequence, frame_indices):\n","\n","    pose_count = pose_sequence.shape[0]\n","    pose_sequence = torch.from_numpy(pose_sequence).to(torch.float32).to(device)\n","    pose_sequence = pose_sequence.reshape(1, pose_count, pose_dim)\n","    pose_sequence_norm = (pose_sequence - pose_mean) / pose_std\n","    pose_sequence_norm = torch.nan_to_num(pose_sequence_norm)\n","\n","    encoder.eval()\n","\n","    latent_vectors = []\n","\n","    seq_excerpt_count = len(frame_indices)\n","\n","    for excerpt_index in range(seq_excerpt_count):\n","        excerpt_start_frame = frame_indices[excerpt_index]\n","        excerpt_end_frame = excerpt_start_frame + sequence_length\n","        excerpt_norm = pose_sequence_norm[:, excerpt_start_frame:excerpt_end_frame, :]\n","\n","        with torch.no_grad():\n","\n","            encoder_output = encoder(excerpt_norm)\n","\n","            encoder_output_mu = encoder_output[0]\n","            encoder_output_std = encoder_output[1]\n","            mu = torch.tanh(encoder_output_mu)\n","            std = torch.abs(torch.tanh(encoder_output_std)) + 0.00001\n","\n","            latent_vector = reparameterize(mu, std)\n","\n","        latent_vector = torch.squeeze(latent_vector)\n","        latent_vector = latent_vector.detach().cpu().numpy()\n","\n","        latent_vectors.append(latent_vector)\n","\n","    encoder.train()\n","\n","    return latent_vectors\n","\n","def decode_sequence_encodings(sequence_encodings, seq_overlap, base_pose):\n","\n","    decoder.eval()\n","\n","    seq_env = np.hanning(sequence_length)\n","    #seq_env = np.concatenate([np.linspace(0.0, 1.0, seq_overlap), np.ones(sequence_length - seq_overlap)])\n","\n","    seq_excerpt_count = len(sequence_encodings)\n","    gen_seq_length = (seq_excerpt_count - 1) * seq_overlap + sequence_length\n","\n","    gen_sequence = np.full(shape=(gen_seq_length, joint_count, joint_dim), fill_value=base_pose)\n","\n","    for excerpt_index in range(len(sequence_encodings)):\n","\n","        #print(\"excerpt_index \", excerpt_index)\n","\n","        latent_vector = sequence_encodings[excerpt_index]\n","        latent_vector = np.expand_dims(latent_vector, axis=0)\n","        latent_vector = torch.from_numpy(latent_vector).to(device)\n","\n","        #print(\"latent_vector s \", latent_vector.shape)\n","\n","        with torch.no_grad():\n","            excerpt_dec_norm = decoder(latent_vector)\n","\n","        excerpt_dec = excerpt_dec_norm * pose_std + pose_mean\n","        excerpt_dec = excerpt_dec.detach().cpu()\n","        excerpt_dec = np.reshape(excerpt_dec, (sequence_length, joint_count, joint_dim))\n","\n","        #print(\"excerpt_dec s \", excerpt_dec.shape)\n","\n","        gen_frame = excerpt_index * seq_overlap\n","\n","        for si in range(sequence_length):\n","\n","            mix = seq_env[si]\n","\n","            current_pose = gen_sequence[gen_frame + si]\n","            target_pose = excerpt_dec[si]\n","\n","            #print(\"si \", si, \" mix \", mix, \"current_pose s \", current_pose.shape, \" target_pose s \", target_pose.shape)\n","\n","            mix_pose = np.add(current_pose * (1.0 - mix) , target_pose * mix)\n","\n","            gen_sequence[gen_frame + si] = mix_pose\n","\n","    gen_sequence = np.reshape(gen_sequence, (gen_seq_length, joint_count, joint_dim))\n","\n","    decoder.train()\n","\n","    return gen_sequence\n","\n","def create_2d_latent_space_representation(sequence_excerpts):\n","\n","    encodings = []\n","\n","    excerpt_count = sequence_excerpts.shape[0]\n","\n","    for eI in range(0, excerpt_count, batch_size):\n","\n","        excerpt_batch = sequence_excerpts[eI:eI+batch_size]\n","\n","        #print(\"excerpt_batch s \", excerpt_batch.shape)\n","\n","        excerpt_batch = torch.from_numpy(excerpt_batch).to(device)\n","\n","        encoder_output = encoder(excerpt_batch)\n","\n","        encoder_output_mu = encoder_output[0]\n","        encoder_output_std = encoder_output[1]\n","        mu = torch.tanh(encoder_output_mu)\n","        std = torch.abs(torch.tanh(encoder_output_std)) + 0.00001\n","\n","        encoding_batch = reparameterize(mu, std)\n","\n","        #print(\"encoding_batch s \", encoding_batch.shape)\n","\n","        encoding_batch = encoding_batch.detach().cpu()\n","\n","        encodings.append(encoding_batch)\n","\n","    encodings = torch.cat(encodings, dim=0)\n","\n","    #print(\"encodings s \", encodings.shape)\n","\n","    encodings = encodings.numpy()\n","\n","    # use TSNE for dimensionality reduction\n","    tsne = TSNE(n_components=2, n_iter=5000, verbose=1)\n","    Z_tsne = tsne.fit_transform(encodings)\n","\n","    return Z_tsne\n","\n","def create_2d_latent_space_image(Z_tsne, highlight_excerpt_ranges, file_name):\n","\n","    Z_tsne_x = Z_tsne[:,0]\n","    Z_tsne_y = Z_tsne[:,1]\n","\n","    plot_colors = [\"green\", \"red\", \"blue\", \"magenta\", \"orange\"]\n","    plt.figure()\n","    fig, ax = plt.subplots()\n","    ax.plot(Z_tsne_x, Z_tsne_y, '-', c=\"grey\",linewidth=0.2)\n","    ax.scatter(Z_tsne_x, Z_tsne_y, s=0.1, c=\"grey\", alpha=0.5)\n","\n","    for hI, hR in enumerate(highlight_excerpt_ranges):\n","        ax.plot(Z_tsne_x[hR[0]:hR[1]], Z_tsne_y[hR[0]:hR[1]], '-', c=plot_colors[hI],linewidth=0.6)\n","        ax.scatter(Z_tsne_x[hR[0]:hR[1]], Z_tsne_y[hR[0]:hR[1]], s=0.8, c=plot_colors[hI], alpha=0.5)\n","\n","        ax.set_xlabel('$c_1$')\n","        ax.set_ylabel('$c_2$')\n","\n","    fig.savefig(file_name, dpi=300)\n","    plt.close()\n","\n","# create latent space plot\n","\n","Z_tsne = create_2d_latent_space_representation(pose_sequence_excerpts)\n","create_2d_latent_space_image(Z_tsne, [], \"latent_space_plot_epoch_{}.png\".format(epochs))\n","\n","# create original sequence\n","\n","orig_sequence = all_motion_data[0][\"/mocap/0/joint/pos_root_zero\"].astype(np.float32)\n","\n","seq_start = 1000\n","seq_length = 1000\n","\n","export_sequence_anim(orig_sequence[seq_start:seq_start+seq_length], \"results/anims/orig_sequence_seq_start_{}_length_{}.gif\".format(seq_start, seq_length))\n","export_sequence_pkl(orig_sequence[seq_start:seq_start+seq_length], \"results/anims/orig_sequence_seq_start_{}_length_{}.pkl\".format(seq_start, seq_length))\n","\n","# recontruct original sequence\n","\n","seq_start = 1000\n","seq_length = 1000\n","seq_overlap = 16 # 2 for 8, 32 for 128\n","base_pose = np.reshape(orig_sequence[0], (joint_count, joint_dim))\n","\n","seq_indices = [ frame_index for frame_index in range(seq_start, seq_start + seq_length, seq_overlap)]\n","\n","seq_encodings = encode_sequences(orig_sequence, seq_indices)\n","gen_sequence = decode_sequence_encodings(seq_encodings, seq_overlap, base_pose)\n","export_sequence_anim(gen_sequence, \"results/anims/rec_sequences_epoch_{}_seq_start_{}_length_{}.gif\".format(epochs, seq_start, seq_length))\n","export_sequence_pkl(gen_sequence, \"results/anims/rec_sequences_epoch_{}_seq_start_{}_length_{}.pkl\".format(epochs, seq_start, seq_length))\n","\n","\n","# random walk in latent space\n","seq_start = 1000\n","seq_length = 1000\n","\n","seq_indices = [seq_start]\n","\n","seq_encodings = encode_sequences(orig_sequence, seq_indices)\n","\n","for index in range(0, seq_length // seq_overlap):\n","    random_step = np.random.random((latent_dim)).astype(np.float32) * 2.0\n","    seq_encodings.append(seq_encodings[index] + random_step)\n","\n","gen_sequence = decode_sequence_encodings(seq_encodings, seq_overlap, base_pose)\n","export_sequence_anim(gen_sequence, \"results/anims/seq_randwalk_epoch_{}_seq_start_{}_length_{}.gif\".format(epochs, seq_start, seq_length))\n","export_sequence_pkl(gen_sequence, \"results/anims/seq_randwalk_epoch_{}_seq_start_{}_length_{}.pkl\".format(epochs, seq_start, seq_length))\n","\n","# sequence offset following\n","\n","seq_start = 1000\n","seq_length = 1000\n","\n","seq_indices = [ seq_index for seq_index in range(seq_start, seq_start + seq_length, seq_overlap)]\n","\n","seq_encodings = encode_sequences(orig_sequence, seq_indices)\n","\n","offset_seq_encodings = []\n","\n","for index in range(len(seq_encodings)):\n","    sin_value = np.sin(index / (len(seq_encodings) - 1) * np.pi * 4.0)\n","    offset = np.ones(shape=(latent_dim), dtype=np.float32) * sin_value * 4.0\n","    offset_seq_encoding = seq_encodings[index] + offset\n","    offset_seq_encodings.append(offset_seq_encoding)\n","\n","gen_sequence = decode_sequence_encodings(offset_seq_encodings, seq_overlap, base_pose)\n","export_sequence_anim(gen_sequence, \"results/anims/seq_offset_epoch_{}_seq_start_{}_length_{}.gif\".format(epochs, seq_start, seq_length))\n","export_sequence_pkl(gen_sequence, \"results/anims/seq_offset_epoch_{}_seq_start_{}_length_{}.pkl\".format(epochs, seq_start, seq_length))\n","\n","\n","# interpolate two original sequences\n","\n","seq1_start = 1000\n","seq2_start = 2000\n","seq_length = 1000\n","\n","seq1_indices = [ seq_index for seq_index in range(seq1_start, seq1_start + seq_length, seq_overlap)]\n","seq2_indices = [ seq_index for seq_index in range(seq2_start, seq2_start + seq_length, seq_overlap)]\n","\n","seq1_encodings = encode_sequences(orig_sequence, seq1_indices)\n","seq2_encodings = encode_sequences(orig_sequence, seq2_indices)\n","\n","mix_encodings = []\n","\n","for index in range(len(seq1_encodings)):\n","    mix_factor = index / (len(seq1_indices) - 1)\n","    mix_encoding = seq1_encodings[index] * (1.0 - mix_factor) + seq2_encodings[index] * mix_factor\n","    mix_encodings.append(mix_encoding)\n","\n","gen_sequence = decode_sequence_encodings(mix_encodings, seq_overlap, base_pose)\n","export_sequence_anim(gen_sequence, \"results/anims/seq_mix_epoch_{}_seq1_start_{}_seq2_start_{}_length_{}.gif\".format(epochs, seq1_start, seq2_start, seq_length))\n","export_sequence_pkl(gen_sequence, \"results/anims/seq_mix_epoch_{}_seq1_start_{}_seq2_start_{}_length_{}.pkl\".format(epochs, seq1_start, seq2_start, seq_length))"],"metadata":{"id":"w2qIpUxT8L_3"},"execution_count":null,"outputs":[]}]}